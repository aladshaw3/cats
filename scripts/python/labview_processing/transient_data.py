## @package transient_data
#
# @brief Read TransientData from CLEERS team
#
# @details Python script to read in CLEERS transient data for
#       NH3 storage on Cu-SSZ-13. This script will store the
#       orginal data as is and provide other functions to
#       redistribute, print, or parse that data as needed.
#
# @author Austin Ladshaw
# @date 02/07/2020
# @copyright This software was designed and built at the Oak Ridge National
#           Laboratory (ORNL) National Transportation Research Center
#           (NTRC) by Austin Ladshaw for research in the catalytic
#           reduction of NOx. Copyright (c) 2020, all rights reserved.
#
# @note The CLEERS data files are very, very large, so I am saving them as *.dat files.
#       The reasoning behind this is so that I can direct 'git' to ignore files that
#       end with a *.dat file extension. This prevents the repository from becoming
#       bloated. The *.dat files behave exactly like regular text files.

import math
import os, sys
from statistics import mean, stdev
import random
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit
from scipy.stats import norm

## Helper function for Normal Distribution
# This function computes the sum of normal distributions. It is used in the transient_data
#   objects in a curve fitting routine to try and determine the contribution of the high
#   and low temperature portions of the TPDs.
#
#   @param x time or temperature value in the TPD
#   @param n number of normal distributions to consider
#   @param p list of params for each normal distribution
#           p[n][0] = avg
#           p[n][1] = std
#           p[n][2] = scale
def normal_sum_func(x, n, p):
    sum = 0
    for i in range(0,n):
        sum += norm.pdf(x, p[i][0], p[i][1])*p[i][2]
    return sum

## Secondary Helper function for curve_fit
# This is the helper function used by the curve_fit routine. It basically takes a list
#   of parameters for a 2-peak distribution and packs those parameters into proper items
#   needed for the normal_sum_func().
def double_peak_normal(x, avg1, std1, scale1, avg2, std2, scale2):
    n = 2
    p = []
    p.append([avg1,std1,scale1])
    p.append([avg2,std2,scale2])
    return normal_sum_func(x, n, p)

## TransientData
# This is the basic object to read, operate on, plot, and save transient CLEERS data
#
# This python class is responsible for reading in a set of tab delimited time series
#   data files generated by the CLEERS Team. Currently, the class is designed to
#   look for some key specific formatting quirks in the data file that is common to
#   all CLEERS data sets (at the moment). Those formatting quirks are the following...
#
#   (1) The first line of every file contains a generic header that contains no data
#   (2) All data contained within the file starts at the second line
#   (3) All data is in a series of columns and each column has a unique header name
#           That header name is used to identify what the data below it is and is
#           used in this object to create a mapping of the data.
#   (4) Every so often, the column names are reinserted into the rows of data.
#           This is used to indicate when a change in the input conditions of
#           a data run has occurred. This class is aware that these changes can
#           and do happen, so it uses that information to parse each set of
#           data (in a same run) under a series of data frames.
#   (5) When a column has a header/name, but no data beneath it, that column is ignored
#   (6) At least one column in the data file must be labeled as "Elapsed Time (unit)"
#          Where 'unit' is any time units.
#
# EXAMPLE:
#
#   first header contains ignored text                  \n
#
#   Elapsed Time (min)  NH3 (300)   NH3 (3000)  T (C)   \n
#
#   0.0                 0.01        1.2         151.2   \n
#
#   0.16                -0.9        2.0         150.5   \n
#
#   Elapsed Time (min)  NH3 (300)   NH3 (3000)  T (C)   \n
#
#   0.32                30.1        34.2        151.6   \n
#
#   0.64                35.6        41.0        149.7   \n
#
#
#   In this example, there are 2 columns containing NH3 data, but is measured by
#   instruments calibrated with different tolerances labeled by the numbers in
#   parentheses. This set also includes the "Elapsed Time (min)" column name as
#   required, then the set of column names repeats halfway down the file. This
#   repeat is how changes in input conditions are marked. Notice that the NH3
#   values suddenly jump at this point. That sudden jump is correlated with
#   the repeat of the column names. The data frames created are then from
#   time 0.0 to 0.16 (as the first frame) and from time 0.32 to 0.64 (as the
#   second frame).
class TransientData(object):
    ## Constructor for the class
    # Initialize data object by passing the current file to it
    # Each key in the data_map represents a column label
    #       Each key maps to a data list
    # The input file should have a specific convention for naming a file
    #       e.g., 20160209-CLRK-BASFCuSSZ13-700C4h-NH3H2Ocomp-30k-0_2pctO2-11-3pctH2O-400ppmNH3-150C.dat
    #
    #       Each important piece of information is split by a "-" character. We then use this to parse
    #   the file name to obtain particular information. However, that file name convention is not necessarily
    #   consistent for all files. So we are limited in what can be interpreted from the names. The most important
    #   information is as follows...
    #
    #       item[2] = Name of the catalyst material
    #       item[3] = Aging condition
    #       item[4] = Run type (TPD, NH3H2Ocomp, etc)
    #       item[5] = flow rate information (in kilo-volumes per hour)
    #       item[-1] = either a temperature or "bp"
    #                   temperature is irrelevant, but bp indicates that this is inlet information
    #                   (Also note, item[-1] will carry the file extension with it)
    #
    #   @param file the name of the data file we are reading
    def __init__(self, file):
        #Check the given file name for any path information and truncate the path information
        file_name = file.split("/")[-1]
        #Parse the file name to gain specific information
        self.input_file_name = file_name
        file_name_info = file_name.split("-")
        ##Name of the material this data applies to
        self.material_name = file_name_info[2]
        ##Type of run for this file
        self.run_type = file_name_info[4]
        if file_name_info[3] == "700C4h":
            self.aging_condition = "De-greened"
            ##Aging time in hours
            self.aging_time = 0
            ##Aging temperature in oC
            self.aging_temp = 700
        else:
            self.aging_condition = file_name_info[3]
            try:
                tem = float(file_name_info[3].split("C")[0])
            except:
                tem = file_name_info[3].split("C")[0]
            self.aging_temp = tem
            if len(file_name_info[3].split("C")) > 1:
                try:
                    tim = float(file_name_info[3].split("C")[1].split("h")[0])
                except:
                    tim = file_name_info[3].split("C")[1].split("h")[0]
            else:
                tim = file_name_info[3]
            self.aging_time = tim
        try:
            ##Space Velocity Flow rate of the experiment (in per hour)
            self.flow_rate = float(file_name_info[5].split("k")[0])*1000
            self.have_flow_rate = True
        except:
            self.flow_rate = file_name_info[5]
            self.have_flow_rate = False
        if file_name_info[-1].split(".")[0] == "bp":
            self.inlet_data = True
            ##Isothermal temperature for the experimental run
            self.isothermal_temp = 0
        else:
            self.inlet_data = False
            try:
                self.isothermal_temp = float(file_name_info[-1].split(".")[0].split("C")[0])
            except:
                self.isothermal_temp = file_name_info[-1].split(".")[0]

        ##Contains data file we are digitizing
        self.data_file = open(file,"r")
        statinfo = os.stat(file)
        ##Contains the first line of the data file
        self.exp_header = ''
        ##Contains a map of all the data by column
        self.data_map = {}
        ##Contains the number of rows of data
        self.num_rows = 0
        ##Contains an ordered list of column names
        self.ordered_key_list = []
        ##Contains an ordered list of the times when experimental inputs changed
        #
        #   NOTE:
        #
        #           this is just used to initialize data in the map, it does not
        #           change or update if the map changes
        self.change_time = []
        ##Contains a list of tuples that hold the start and end time for each
        #   instance in the data where inputs were changed. This item may be
        #   slightly redundant with change_time, but will be more useful to
        #   end users who might want to create separate plots for each time frame
        self.time_frames = []
        ##Contains a map of the inputs values that correspond to change_time
        #
        #   Keys of this map are modifications to the keys of data_map that
        #   correspond to output values corresponding to the input values given
        #
        #   Specific values on input can be user specified or auotmatically generated
        #   based on the last few data points in the corresponding columns.
        #
        #   NOTE:
        #
        #       When pairing bypass data with result data, this object will not be
        #       used unless the data alignment function needs it.
        self.input_change = {}
        ##Contains the name of the time key for the data_map
        #
        #   Key must contain "Elapsed Time (min)" in the data file
        #   unit can be any time units, but the first part of the string
        #   must alwas exist as "Elapsed Time (..."
        #
        #       (NOTE: We assume that elapsed time has units of minutes)
        self.time_key = ""
        ##Volume of the system over which data was gathered (in L)
        #
        #   Space volume of the column for the catalyst
        #   User must manually assign a value, if needed
        self.sys_vol = 0.015708
        ##Void volume fraction for the total system volume
        #
        #   This would represent the overall bulk porosity of the
        #   catalyst. User must manually override this value if needed.

        self.void_frac = 0.3309
        if statinfo.st_size >= 10000000:
            print("\nReading " + str(statinfo.st_size/1E6) + " MB file. Please wait...")
        self.readFile()
        if statinfo.st_size >= 10000000:
            print("Finished!")

    ## Function to print object information to the console
    def __str__(self):
        message = "\nFile Name:\t" + self.data_file.name
        message += "\nMaterial:\t" + self.material_name
        message += "\nAging Condition:\t" + self.aging_condition
        message += "\nFlow Rate (hr^-1):\t" + str(self.flow_rate)
        message += "\nInlet Conditions:\t"
        if self.inlet_data == True:
            message += "True"
        else:
            message += "False"
            message += "\nIsothermal Temp (C):\t" + str(self.isothermal_temp)
        message += "\nFile Header:\t" + self.exp_header
        message += "\nNumber of Columns:\t" + str(len(self.data_map))
        message += "\nNumber of Rows:\t" + str(self.num_rows)
        return message + "\n"

    ## Function to display the column names to the console.
    # This method is particularly useful when using this python script
    # interactively as it will tell you all the columns by name that you
    # can have access to. To get a specific column, or other functions
    # that require a specific column, you use the names of the columns
    # displayed by this function.
    def displayColumnNames(self):
        print(self.data_map.keys())

    ## Function to read in the data file
    # User does not need to call this function separately, the constructor
    # calls this function for you.
    def readFile(self):
        i = 0
        first_data_line = False
        has_read_map = {}
        for line in self.data_file:
            line_list = line.split('\t')
            #Ignore the first line of the data file and stores as header
            if (i == 0):
                self.exp_header = line

            # This is the first real header of the data
            if (i == 1):
                for item in line_list:
                    #Ignore the ending character
                    if item != '\n':
                        #Find the time_key by parsing and checking item
                        if item.strip().split("(")[0] == "Elapsed Time ":
                            self.time_key = item.strip()
                        #NOTE: The registered keys will be the column names stripped of leading and trailing whitespaces
                        self.data_map[item.strip()] = []
                        has_read_map[item.strip()] = False
                        self.ordered_key_list.append(item.strip())
                    else:
                        # Force a new column for input conditions
                        self.change_time.append(0)

            if i == 2:
                first_data_line = True
            else:
                first_data_line = False
            if (i > 1): #or greater
                n = 0
                for item in line_list:
                    #Check item to make sure that we do not get another column key
                    # If we do, then do not append data to map, instead record
                    #   the data_map[time_key] value from prior as the point
                    #   when input conditions changed
                    #NOTE: Check the stripped items for keys
                    if item.strip() in self.data_map.keys():
                        if (n == 0):
                            self.change_time.append(float(self.data_map[self.time_key][-1]))
                    # If we don't, then record the data into the map
                    else:
                        #Ignore ending character
                        if item != '\n':
                            if has_read_map[self.ordered_key_list[n]] == False:
                                #Attempt to store data as a number
                                try:
                                    self.data_map[self.ordered_key_list[n]].append(float(item))
                                #Store data as string if it is not a number
                                except:
                                    if first_data_line == True:
                                        self.data_map[self.ordered_key_list[n]].append(item)
                                    else:
                                        if len(self.data_map[self.ordered_key_list[n]]) > 0:
                                            if type(self.data_map[self.ordered_key_list[n]][-1]) is not int and type(self.data_map[self.ordered_key_list[n]][-1]) is not float:
                                                self.data_map[self.ordered_key_list[n]].append(item)
                                has_read_map[self.ordered_key_list[n]] = True
                    n+=1
                for item in has_read_map:
                    has_read_map[item] = False
            i+=1
        #END of line loop
        self.num_rows = len(self.data_map[self.time_key])
        for i in range(1,len(self.change_time)):
            self.time_frames.append((self.change_time[i-1],self.change_time[i]))
        self.time_frames.append((self.change_time[-1],self.data_map[self.time_key][-1]))
        self.closeFile()

    ## Function to manually close the open data file
    def closeFile(self):
        self.data_file.close()

    ## Function to set the system volume
    def defineVolume(self, vol):
        try:
            self.sys_vol = float(vol)
        except:
            print("Error! Given value is non-numeric!")

    ## Function to set the system porosity
    def defineVoidFraction(self, frac):
        try:
            val = float(frac)
            if val > 1:
                val = 1
            if val < 0:
                print("Error! Cannot give negative void fraction")
                return
            self.void_frac = val
        except:
            print("Error! Given value is non-numeric!")

    ## This function will add a column to the data map given the column name and associated data
    #
    #   NOTE:
    #       Appending the column does NOT copy the column into the map. It merely directs
    #       the map to point to the given data_set. If you change the data_set that you pass
    #       to this function, then the data in this object's map will also change.
    def appendColumn(self, column_name, data_set):
        #First, check to make sure the map has been prepared
        if (len(self.data_map) == 0):
            print("Error! File has not been read and stored!")
            return

        #Make sure the data_set is actually a list of data
        if type(data_set) is not list:
            print("Error! The data_set must be a list of data!")
            return

        #Next, check to make sure the column_name is not already in the map
        if column_name in self.data_map.keys():
            if len(self.data_map[column_name]) != len(data_set):
                print("Error! That data column is already in the structure and sizes are mis-matched!")
                return

        #Lastly, check to make sure the length of the data_set matches the length of the time series data
        if len(self.data_map[self.time_key]) != len(data_set):
            print("Error! The data set size does not match the existing data set size!")
            return

        self.data_map[column_name] = data_set

    ## This function is used to create a step input column based on data frames
    #
    # @param column_name Name of the column to append to the data
    # @param column_value_list List of column data values for the corresponding data frame windows
    def appendColumnByFrame(self, column_name, column_value_list):
        # First, check to make sure the column name is not in the data_map
        if column_name in self.data_map.keys():
            print("Error! That data column already exists!")
            return
        # Next, check to make sure the column_value_list is the same length as the time_frames
        if len(column_value_list) != len(self.time_frames):
            print("Error! The size of the column_value_list does not match the data frame size!")
            return

        #Loop through all time values
        self.data_map[column_name] = []
        index = 0
        for time in self.data_map[self.time_key]:
            if time > self.time_frames[index][1]:
                index+=1
            self.data_map[column_name].append(column_value_list[index])


    ## This function will extract column sets from the data_map and return a new, reduced map
    #
    #   NOTE:
    #
    #       Column list must be a list of valid keys in the data_map
    def extractColumns(self, column_list):
        new_map = {}
        if type(column_list) is list:
            for item in column_list:
                #Check to make sure the item is a key in data_map
                if item in self.data_map.keys():
                    new_map[item] = self.data_map[item]
                else:
                    print("Error! Invalid Key!")
            #End item loop
        else:
            if column_list in self.data_map.keys():
                new_map[column_list] = self.data_map[column_list]
            else:
                print("Error! Invalid Key!")
        return new_map

    ## This function is used to set all negative observations to zero for a given column or list of columns
    #  The provided column_list argument can either be a list of columns in the map, or the name of 1 column
    #  All negative values are replaced with zeros.
    #
    # @param column_list Name or list of names of the columns to remove negative values from
    def removeNegatives(self, column_list):
        if type(column_list) is list:
            for item in column_list:
                #Check to make sure the item is a key in data_map
                if item in self.data_map.keys():
                    i=0
                    for value in self.data_map[item]:
                        if value < 0:
                            self.data_map[item][i] = 0
                        i+=1
                else:
                    print("Error! Invalid Key!")
        else:
            if column_list in self.data_map.keys():
                i=0
                for value in self.data_map[column_list]:
                    if value < 0:
                        self.data_map[item][i] = 0
                    i+=1
            else:
                print("Error! Invalid Key!")

    ## This function is used to perform a number of operations using a given column
    #   The default setting is to operate on the given column to change it's values,
    #   however, the user can specify that the result should create a new column to
    #   append to the map if desired.
    #
    #   Supported Operations:
    #
    #       multiply by a scalar ==> this_column, "*", constant
    #
    #       divide by a scalar   ==> this_column, "/", constant
    #
    #       add a scalar         ==> this_column, "+", constant
    #
    #       subtract a scalar    ==> this_column, "-", constant
    #
    #       multiply by a column ==> this_column, "*", other_column
    #
    #       divide by a column   ==> this_column, "/", other_column
    #
    #       add a column         ==> this_column, "+", other_column
    #
    #       subtract a column    ==> this_column, "-", other_column
    #
    #   NOTE:
    #
    #           Multiplying and dividing by a given column will just take all
    #           corresponding rows of the first column and perform the operation
    #           using the data in the other corresponding row of the other column.
    #
    #   EXAMPLES:
    #
    #       obj.mathOperation("A","*","B")
    #
    #               this will compute the result of the multiplication of every
    #                   row of A times every row of B and store the result in A
    #
    #       obj.mathOperation("A","+",273,True)
    #
    #               this will compute the result of every row of A plus 273 and
    #                   store the result in a new column in the map named "A+273"
    #
    #       obj.mathOperation("A","/","B",True,"Res")
    #
    #               this will compute the result of the division of every row
    #                   of A divided by every row of B and store the result in a
    #                   new column in the map named "Res"
    def mathOperation(self, column_name, operator, value_or_column, append_new = False, append_name = ""):
        if column_name not in self.data_map.keys():
            print("Error! Unrecognized column_name... " + column_name)
            return
        if type(operator) is not str:
            print("Error! Given operator must be a string...")
            print("\tCurrent Options: '*', '/', '+', and '-' ")
            return
        if operator != "*" and operator != "/" and operator != "+" and operator != "-":
            print("Error! Unsupported operator...")
            return
        if type(self.data_map[column_name][0]) is not int and type(self.data_map[column_name][0]) is not float:
            print("Error! Non-numeric data in the given first column...")
            return

        #Check the value_or_column variable type
        if type(value_or_column) is str:
            if value_or_column not in self.data_map.keys():
                print("Error! Argument given as the value is not a valid key...")
                return
            if type(self.data_map[value_or_column][0]) is not int and type(self.data_map[value_or_column][0]) is not float:
                print("Error! Non-numeric data in the given second column...")
                return
            if append_new == True:
                new_name = append_name
                if new_name == "":
                    new_name = column_name + operator + value_or_column
                self.data_map[new_name] = []
                i=0
                for value in self.data_map[column_name]:
                    if operator == "*":
                        self.data_map[new_name].append(value*self.data_map[value_or_column][i])
                    elif operator == "/":
                        self.data_map[new_name].append(value/self.data_map[value_or_column][i])
                    elif operator == "-":
                        self.data_map[new_name].append(value-self.data_map[value_or_column][i])
                    elif operator == "+":
                        self.data_map[new_name].append(value+self.data_map[value_or_column][i])
                    else:
                        print("Error! How did you even get here...?")
                        return
                    i+=1
            else:
                i=0
                for value in self.data_map[column_name]:
                    if operator == "*":
                        self.data_map[column_name][i] = (value*self.data_map[value_or_column][i])
                    elif operator == "/":
                        self.data_map[column_name][i] = (value/self.data_map[value_or_column][i])
                    elif operator == "-":
                        self.data_map[column_name][i] = (value-self.data_map[value_or_column][i])
                    elif operator == "+":
                        self.data_map[column_name][i] = (value+self.data_map[value_or_column][i])
                    else:
                        print("Error! How did you even get here...?")
                        return
                    i+=1
        else:
            if append_new == True:
                new_name = append_name
                if new_name == "":
                    new_name = column_name + operator + str(value_or_column)
                self.data_map[new_name] = []
                i=0
                for value in self.data_map[column_name]:
                    if operator == "*":
                        self.data_map[new_name].append(value*value_or_column)
                    elif operator == "/":
                        self.data_map[new_name].append(value/value_or_column)
                    elif operator == "-":
                        self.data_map[new_name].append(value-value_or_column)
                    elif operator == "+":
                        self.data_map[new_name].append(value+value_or_column)
                    else:
                        print("Error! How did you even get here...?")
                        return
                    i+=1
            else:
                i=0
                for value in self.data_map[column_name]:
                    if operator == "*":
                        self.data_map[column_name][i] = (value*value_or_column)
                    elif operator == "/":
                        self.data_map[column_name][i] = (value/value_or_column)
                    elif operator == "-":
                        self.data_map[column_name][i] = (value-value_or_column)
                    elif operator == "+":
                        self.data_map[column_name][i] = (value+value_or_column)
                    else:
                        print("Error! How did you even get here...?")
                        return
                    i+=1

    ##This function will extract a row of data (or set of rows) based on the value of Elapsed time provided
    def extractRows(self, min_time, max_time):
        new_map = {}
        for item in self.data_map:
            new_map[item] = []
        #loop through all time data
        n = 0
        for time in self.data_map[self.time_key]:
            if time > max_time:
                break
            if time >= min_time:
                for item in self.data_map:
                    if len(self.data_map[item]) > n:
                        new_map[item].append(self.data_map[item][n])
            n+=1

        return new_map

    ## This function will get a particular data point based on the given elapsed time and column name
    def getDataPoint(self, time_value, column_name):
        point = 0
        if column_name not in self.data_map.keys():
            print("Error! Invalid Key!")
            return
        n = 0
        start_time = 0
        start_point = 0
        end_time = 0
        end_point = 0
        if time_value >= self.data_map[self.time_key][-1]:
            return self.data_map[column_name][-1]
        #What should we do if we reach beyond the total time?
        for time in self.data_map[self.time_key]:
            if time > time_value:
                if n == 0:
                    start_time = 0
                    start_point = self.data_map[column_name][n]
                else:
                    start_time = self.data_map[self.time_key][n-1]
                    start_point = self.data_map[column_name][n-1]
                end_time = time
                end_point = self.data_map[column_name][n]
                break
            n+=1
        #Perform linear interpolation between start_point and end_point
        if type(end_point) is not int and type(end_point) is not float:
            point = end_point
        else:
            try:
                point = (end_point - start_point)/(end_time - start_time)*(time_value - start_time) + start_point
            except:
                point = end_point
        return point

    ##Function to retrive the maximum value in a given column
    def getMaximum(self, column_name):
        if column_name not in self.data_map.keys():
            print("Error! Invalid Key!")
            return
        return max(self.data_map[column_name])

    ##Function to retrive the minimum value in a given column
    def getMinimum(self, column_name):
        if column_name not in self.data_map.keys():
            print("Error! Invalid Key!")
            return
        return min(self.data_map[column_name])

    ##Function to calculate and retrive the average value in a given column
    def getAverage(self, column_name):
        if column_name not in self.data_map.keys():
            print("Error! Invalid Key!")
            return
        try:
            return mean(self.data_map[column_name])
        except:
            print("Error! Non-numeric data set has no average value!")
            return

    ##Function to retrive the range of the data in a given column (i.e., max - min point)
    def getDataRange(self, column_name):
        return self.getMaximum(column_name) - self.getMinimum(column_name)

    ##Function to return the list of time ranges
    def getTimeFrames(self):
        return self.time_frames

    ##Function to return the number of rows
    def getNumRows(self):
        return self.num_rows

    ##Function to return the number of columns
    def getNumCols(self):
        return len(self.data_map)

    ##This function will iterate through all columns to find data that can be compressed or eliminated
    #
    #       For instance,
    #
    #                       if a column contains no data, then delete it
    #
    #                       if there are multiple columns that carrier similar info, then combine them
    #
    #       USAGE:
    #
    #                       When we have sets of data that are measuring the same thing, but at
    #                       different levels of tolerances, we can use this function to select the
    #                       best information to keep based on those tolerances.
    #
    #   EXAMPLE:
    #
    #       A column NH3 (300) and another column NH3 (3000) would be combined into a single
    #       column named NH3 (300,3000) and use the data from either of the two columns depending
    #       on whether or not the recorded data went over the tolerance of the instrument and which
    #       of the recordings were closest to that tolerance level.
    def compressColumns(self):
        #Iterate through the data map to find information to delete immediately
        list_to_del = []
        for item in self.data_map:
            if len(self.data_map[item]) < len(self.data_map[self.time_key]):
                list_to_del.append(item)
        for item in list_to_del:
            del self.data_map[item]

        #Iterate through the map and find compressible columns
        frac_keys = {}  #Map of a list of columns that can compress
        for item in self.data_map:
            first = item.split("(")
            if len(first) > 1:
                last = first[1].split(")")
            else:
                last = ""
            if first[0] in frac_keys.keys():
                #Value check is used to determine whether or not the duplicated key
                #       is significant or can be neglected. If it is significant, then
                #       the value will be a number. Otherwise, the columns can't be merged.
                try:
                    val = float(last[0])
                    frac_keys[first[0]].append(item)
                except:
                    val = 0
            else:
                frac_keys[first[0]] = []
                frac_keys[first[0]].append(item)
        #Any frac_keys[key] ==> list that has a length > 1 can be compressed
        #       Need to add new key to data_map to hold compressed data, then
        #       delete the old keys to reduce the map size.
        for item in frac_keys:
            if len(frac_keys[item]) > 1:
                val_list = []
                #NOTE: this line is solely for getting my code editor to recognize function names as symbols
                o="()"  #Symbol-tree-viewer does not like unclosed parentheses, even in quotes
                new_name = item +o.split(")")[0]
                j=0
                for sub_key in frac_keys[item]:
                    first = sub_key.split("(")
                    last = first[1].split(")")
                    val_list.append(float(last[0]))
                    if j == 0:
                        new_name += str(int(val_list[j]))
                    else:
                        new_name += "," + str(int(val_list[j]))
                    j+=1
                new_name += ")"
                self.data_map[new_name] = []
                #Now loop through all rows and insert proper data into new column
                n = 0
                for value in self.data_map[frac_keys[item][0]]:
                    #value we are on here corresponds to the val_list[0] limit
                    #Loop through all other columns that can compress and
                    #find value to register based on limits in val_list
                    dist = []
                    dist.append(val_list[0] - value)
                    for i in range(1,len(frac_keys[item])):
                        dist.append(val_list[i] - self.data_map[frac_keys[item][i]][n])

                    #The index of the smallest postive dist is the index i for the data to append
                    reg_index = 0
                    old_d = dist[0]
                    for i in range(1,len(frac_keys[item])):
                        if dist[i] > 0 and old_d > 0:
                            if dist[i] < old_d:
                                reg_index = i
                                old_d = dist[i]
                        elif old_d < 0:
                            old_d = dist[i]
                            reg_index = i
                    self.data_map[new_name].append(self.data_map[frac_keys[item][reg_index]][n])
                    n+=1
                #End value loop

                #Now, delete the original columns
                for sub_key in frac_keys[item]:
                    del self.data_map[sub_key]
            #End if


    ##This function will delete the given columns from the map
    def deleteColumns(self, column_list):
        #NOTE: column_list is either a list of columns or a single column_name
        if type(column_list) is list:
            #iterate through list and delete any columns that can be deleted
            for item in column_list:
                if item in self.data_map.keys():
                    del self.data_map[item]
                else:
                    return
        else:
            #delete only the given column
            if column_list in self.data_map.keys():
                del self.data_map[column_list]
            else:
                return

    ##This function will delete all columns in the data_map except for the ones specified to retain
    def retainOnlyColumns(self, column_list):
        #First, check to make sure that the columns named in the list are valid
        keep = {}
        if type(column_list) is list:
            for item in column_list:
                if item not in self.data_map.keys():
                    print("Warning! Invalid Column Name! No further action taken to delete columns...")
                    print("\tName given: "+str(item))
                else:
                    keep[item] = 0
        else:
            if column_list not in self.data_map.keys():
                print("Warning! Invalid Column Name! No further action taken to delete columns...")
                print("\tName given: "+str(item))
            else:
                keep[column_list] = 0

        #Create list of columns to delete
        list_to_del = []
        for item in self.data_map:
            if item not in keep.keys():
                list_to_del.append(item)

        #Delete items in the list
        for item in list_to_del:
            del self.data_map[item]

    ##This function is used to print processed data to an output file
    def printAlltoFile(self, file_name = ""):
        if file_name == "":
            file_name = self.input_file_name.split(".")[0]+"-output.dat"
        file = open(file_name,'w')
        file.write(str(self))
        file.write("\n")
        i=0
        first = ""
        for item in self.data_map:
            if i == 0:
                file.write(str(item))
                first = str(item)
            else:
                file.write("\t"+str(item))
            i+=1
        file.write("\n")
        j=0
        for value in self.data_map[first]:
            i = 0
            for item in self.data_map:
                if i == 0:
                    file.write(str(self.data_map[item][j]))
                else:
                    file.write("\t"+str(self.data_map[item][j]))
                i+=1
            file.write("\n")
            j+=1
        file.close()

    ##This function is used to print out the equilibrium information in each column for each time frame
    def printEquilibriaTimeFrames(self, file_name = "", avg_points = 10):
        if file_name == "":
            file_name = self.input_file_name.split(".")[0]+"-Equilibria.dat"
        file = open(file_name,'w')
        file.write(str(self))
        file.write("\n")
        equ_map = {}
        for item in self.data_map:
            if item != self.time_key:
                equ_map[item] = [0.0]*len(self.time_frames)

        # Loop through each item
        for item in self.data_map:
            if item != self.time_key:
                # Loop through each time in reverse
                points = 0
                value_sum = 0.0
                has_calc = False
                index = len(self.time_frames)-1
                time_index = len(self.data_map[self.time_key])-1
                for time in reversed(self.data_map[self.time_key]):
                    if points >= avg_points:
                        #Run Calculation
                        if has_calc == False:
                            value_sum = value_sum/avg_points
                            has_calc = True
                            if value_sum < 0:
                                value_sum = 0
                            equ_map[item][index] = value_sum
                        value_sum = 0
                        #Update only if in next time bin
                        if self.time_frames[index][0] >= time:
                            index = index - 1
                            value_sum += self.data_map[item][time_index]
                            has_calc = False
                            points = 1
                    else:
                        #grab more data
                        value_sum += self.data_map[item][time_index]
                        points += 1
                    time_index = time_index - 1
                #End time loop
        #End item loop

        #Iterate through equ_map and print to file
        i=0
        first = ""
        file.write(str("Time Frame")+"\t")
        for item in equ_map:
            if i == 0:
                file.write(str(item))
                first = str(item)
            else:
                file.write("\t"+str(item))
            i+=1
        file.write("\n")
        j=0
        for value in equ_map[first]:
            i = 0
            file.write(str(self.time_frames[j])+"\t")
            for item in equ_map:
                if i == 0:
                    file.write(str(equ_map[item][j]))
                else:
                    file.write("\t"+str(equ_map[item][j]))
                i+=1
            file.write("\n")
            j+=1

        file.close()

    ##This function is used to print select columns of data to a file
    def printColumnstoFile(self, column_list, file_name = ""):
        if type(column_list) is not list:
            print("Error! You must provide a list of columns to print to a file!")
            return
        for name in column_list:
            if name not in self.data_map.keys():
                print("Error! Invalid column names given...")
                return
        if file_name == "":
            file_name = self.input_file_name.split(".")[0]+"-SelectedOutput.dat"
        file = open(file_name,'w')
        file.write(str(self))
        file.write("\n")
        i=0
        for name in column_list:
            if i == 0:
                file.write(str(name))
            else:
                file.write("\t"+str(name))
            i+=1
        file.write("\n")
        j=0
        for value in self.data_map[column_list[0]]:
            i = 0
            for name in column_list:
                if i == 0:
                    file.write(str(self.data_map[name][j]))
                else:
                    file.write("\t"+str(self.data_map[name][j]))
                i+=1
            file.write("\n")
            j+=1
        file.close()

    ##Function to create an approximate rate_map and return it for further processing or printing
    #
    #   This function will take in user arguments to construct approximate rate calculations of
    #   columns in the data_map. The rate calculations are all approximations of the time derivatives
    #   of the given columns. Time derivatives are approximated with a centered difference using
    #   n-1, n, and n+1 points.
    #
    #       df/dt = f(n+1) - f(n-1) / ( t_n+1 - t_n-1 )
    #
    #   The n-1, n, and n+1 points are constructed from 21 neighboring points in order to more accurately
    #   estimate the smooth rates of change when the data is relatively noisy. Thus, to approximate a single
    #   rate requires 63 data points.
    #
    #   @param column_list list of column names to calculate rates for (can be list or single name)
    #   @param max_count maximum number of data points to aggregate to form n-1, n, and n+1
    def createRateMap(self, column_list = [], max_count = 21):
        rate_map = {}
        col_list = []
        if len(column_list) == 0:
            for item in self.data_map:
                column_list.append(item)

        if type(column_list) is list:
            col_list = column_list
        else:
            if column_list not in self.data_map.keys():
                print("Error! Invalid column name!")
                return
            else:
                col_list.append(column_list)

        for item in col_list:
            rate_map[item] = []

        #Forces the rate map to contain the time key
        rate_map[self.time_key] = []

        #Loop over all items in the rate_map
        for item in rate_map:
            #Start by looping through the current rows
            count = 0
            fn=0
            j=0
            while j<self.getNumRows():
                if count < max_count:
                    fn+=self.data_map[item][j]
                    count+=1
                else:
                    fn = fn/max_count
                    rate_map[item].append(fn)
                    fn=0
                    count=0
                j+=1
            #End While loop
        #End map loop

        #Loop again to over the rate_map to approximate time derivatives
        time_der_map = {}
        for item in rate_map:
            #if item == self.time_key:
            #    break
            name ="d{"+item+"}/dt"
            time_der_map[name] = [0.]*len(rate_map[self.time_key])

            n=1
            while n<len(time_der_map[name])-1:
                time_der_map[name][n] = (rate_map[item][n+1] - rate_map[item][n-1]) / (rate_map[self.time_key][n+1] - rate_map[self.time_key][n-1])
                n+=1

            time_der_map[name][0] = (rate_map[item][1] - rate_map[item][0]) / (rate_map[self.time_key][1] - rate_map[self.time_key][0])
            time_der_map[name][len(time_der_map[name])-1] = (rate_map[item][len(time_der_map[name])-1] - rate_map[item][len(time_der_map[name])-2]) / (rate_map[self.time_key][len(time_der_map[name])-1] - rate_map[self.time_key][len(time_der_map[name])-2])

        for item in time_der_map:
            rate_map[item] = time_der_map[item]

        return rate_map

    ##Function to a create plot from the data_map
    #
    #   Options:
    #
    #       - column_list: list of columns to create plots of (default is all columns of plottable data)
    #
    #       - range: tuple of the minimum to maximum time values that you want plotted (default is full range)
    #
    #       - display: if True, the images will be displayed once complete
    #
    #       - save: if True, the images will be saved to a file
    #
    #       - file_name: name of the file to save the plot to
    #
    #       - file_type: type of image file to save as (default = .png)
    #                       allowed types: .png, .pdf, .ps, .eps and .svg
    #
    #       - x_col:    name of the column to plot the data against
    #                       if left None, then x_col defaults to the time column
    def createPlot(self, column_list = [], range=None, display=False, save=True, file_name="",file_type=".png",subdir="",x_col=None):
        if file_type != ".png" and file_type != ".pdf" and file_type != ".ps" and file_type != ".eps" and file_type != ".svg":
            print("Warning! Unsupported image file type...")
            print("\tDefaulting to .png")
            file_type = ".png"
        if range == None:
            range = [self.data_map[self.time_key][0],self.data_map[self.time_key][-1]]
        if type(range) is not list and type(range) is not tuple:
            print("Error! The range argument must be a list or tuple!")
            return
        #Check to see if folder exists and create if needed
        if subdir != "" and not os.path.exists(subdir) and save == True:
            os.makedirs(subdir)
            subdir+="/"
        if x_col == None:
            x_col = self.time_key
        else:
            if x_col not in self.data_map.keys():
                print("Error! Invalid column name!")
                return
        if type(column_list) is list:
            if len(column_list) == 0:
                fig = plt.figure()
                xvals = self.extractRows(range[0],range[1])[x_col]
                leg = []
                ylab = ""
                #if column_list is empty, then plot all data columns
                if len(self.data_map)-1 > 3:
                    print("Warning! Plotting all data to single graph will result in visualization problems...")
                i=0
                for item in self.data_map:
                    if item != x_col:
                        yvals = self.extractRows(range[0],range[1])[item]
                        plt.plot(xvals,yvals)
                        leg.append(item)
                        if i==0:
                            ylab += item
                        else:
                            ylab += "\n"+item
                        i+=1
                plt.legend(leg)
                plt.xlabel(x_col)
                plt.ylabel(ylab)
                plt.tight_layout()
                if file_name == "":
                    file_name = 'AllData'
                if save == True:
                    plt.savefig(subdir+file_name+'-Plot'+file_type)
                if display == True:
                    fig.show()
                    print("\nDisplaying plot. Press enter to continue...(this closes the images)")
                    input()
                plt.close()
            else:
                fig = plt.figure()
                xvals = self.extractRows(range[0],range[1])[x_col]
                leg = []
                ylab = ""
                name = ""
                #otherwise, plot all columns given (except for the time column)
                if len(column_list)-1 > 3:
                    print("Warning! Plotting all data to single graph will result in visualization problems...")
                i=0
                for column in column_list:
                    if column not in self.data_map.keys():
                        print("Warning! Invalid column name given in column_list!")
                        print("\tSkipping " + column)
                    else:
                        if column != x_col:
                            yvals = self.extractRows(range[0],range[1])[column]
                            plt.plot(xvals,yvals)
                            leg.append(column)
                            name += column + "-"
                            if i==0:
                                ylab += column
                            else:
                                ylab += "\n"+column
                            i+=1
                plt.legend(leg)
                plt.xlabel(x_col)
                plt.ylabel(ylab)
                plt.tight_layout()
                if file_name == "":
                    i=0
                    for sec in name.split("/"):
                        if i==0:
                            file_name += sec
                        else:
                            file_name += "_per_" + sec
                        i+=1
                if save == True:
                    plt.savefig(subdir+file_name+'Plot'+file_type)
                if display == True:
                    fig.show()
                    print("\nDisplaying plot. Press enter to continue...(this closes the images)")
                    input()
                plt.close()
        else:
            if column_list not in self.data_map.keys():
                print("Error! Invalid column name!")
                return
            else:
                if column_list != x_col:
                    fig = plt.figure()
                    xvals = self.extractRows(range[0],range[1])[x_col]
                    leg = []
                    yvals = self.extractRows(range[0],range[1])[column_list]
                    plt.plot(xvals,yvals)
                    leg.append(column_list)
                    name = column_list + "-"
                    ylab = column_list
                    plt.legend(leg)
                    plt.xlabel(x_col)
                    plt.ylabel(ylab)
                    plt.tight_layout()
                    if file_name == "":
                        i=0
                        for sec in name.split("/"):
                            if i==0:
                                file_name += sec
                            else:
                                file_name += "_per_" + sec
                            i+=1
                    if save == True:
                        plt.savefig(subdir+file_name+'Plot'+file_type)
                    if display == True:
                        fig.show()
                        print("\nDisplaying plot. Press enter to continue...(this closes the images)")
                        input()
                    plt.close()

    ##Quick use function for saving all processed data plots in a series of output files
    #   File names will be automatically generated and plots will not be displayed live
    def savePlots(self, range=None, folder="", file_type=".png", x_col=None):
        input_list = []
        result_list = []
        for item in self.data_map:
            if "[input]" in item and item != self.time_key:
                input_list.append(item)
            else:
                if item != self.time_key:
                    result_list.append(item)
        pairs = []
        for item1 in input_list:
            for item2 in result_list:
                if item1.split("[input]")[0] in item2:
                    pairs.append([item1,item2])
        for pair in pairs:
            if pair[1] in result_list:
                result_list.remove(pair[1])

        #Print to a folder if possible
        if folder == "":
            if range != None:
                folder = self.input_file_name.split(".")[0]+"-range("+str(int(range[0]))+"-"+str(int(range[1]))+")Plots/"
            else:
                folder = self.input_file_name.split(".")[0]+"-range(All)"+"Plots/"
        #Check to see if folder exists and create if needed
        if not os.path.exists(folder):
            os.makedirs(folder)
        #iterate through all pairs and columns to call the createPlot function
        for pair in pairs:
            self.createPlot(pair,range,False,True,"",file_type,folder, x_col)
        for solo in result_list:
            self.createPlot(solo,range,False,True,"",file_type,folder, x_col)

    ##Function to iteratively save all plots in all time frames separately
    def saveTimeFramePlots(self, folder="", file_type=".png", x_col=None):
        #First, save the full time ranges
        self.savePlots(None,folder,file_type, x_col)
        #Then iterate through all time frames to save separately
        for frame in self.time_frames:
            self.savePlots(frame,folder,file_type, x_col)



    ##This function compresses the rows of the data map
    #   New columns are created to store compressed data and old columns are deleted to reserve space
    #
    #   The 'factor' argument is optional and is used to determine how much compression to use
    #
    #       Default is 2x compression: ==>  Cuts number of rows in half
    def compressRows(self, factor = 2):
        new_key_list = {}   #Map that links old key values (item) to the new keys
        factor = int(factor)
        for item in self.data_map:
            new_key = str(item) + "-" + str(int(factor)) + "x Compression"
            new_key_list[item] = new_key

        #time_key is forced to change here
        self.time_key += "-" + str(int(factor)) + "x Compression"

        #Compression will work by reading the first 'factor' # of values from a list and averaging them
        #   That average value becomes the new value to place into the new data_map with a new key
        #   This cycle repeats until no data is left to compress
        for item in new_key_list:
            self.data_map[new_key_list[item]] = []
            i = 0
            reset = 1
            avg = 0
            for value in self.data_map[item]:
                if type(value) is not int and type(value) is not float:
                    if reset == factor:
                        reset = 1
                        self.data_map[new_key_list[item]].append(value)
                    else:
                        reset+=1
                else:
                    if reset == factor:
                        avg += value
                        avg = avg/float(factor)
                        reset = 1
                        self.data_map[new_key_list[item]].append(avg)
                        avg = 0
                    else:
                        avg+=value
                        reset+=1
                i+=1
            del self.data_map[item]
        self.num_rows = len(self.data_map[self.time_key])


    ##This function will take in a data_map key name and a list of changed values
    #   to add to the map of input_change for each input that corresponds to an
    #   output in data_map for the size of the change_time list
    #
    #       NOTE:
    #
    #               Make sure you give the same units for the data in the input_list
    #               as the units provided in the corresponding output in data_map
    def registerChangedInput(self, data_key, input_list):
        #First, check to make sure that the data_key is valid
        if data_key not in self.data_map.keys():
            print("Error! No corresponding output value exists in data_map!")
            return

        #Next, check to make sure that the input_list size is the same as the change_time size
        if len(input_list) != len(self.change_time):
            print("Error! List of given changed inputs does not match length of change_time")
            return

        self.input_change[data_key+'[input]'] = input_list

    ## This function automates the above function by utiliizing the corresponding
    #   output information of the given data_key to automatically approximate the
    #   input data for each change_time. That input data is estimated by averaging
    #   the last few output data points within the corresponding time range.
    #   By default, the last few data points are taken as the last 10 data points,
    #   however, you can override this by simply calling this function with a
    #   different value for avg_points. You may also specify whether or not the
    #   inlet conditions for this data set should be non-negative (e.g., for things
    #   such as inlet concentrations or molefractions)
    def autoregChangedInput(self, data_key, avg_points = 10, non_neg = True):
        #First, check to make sure that the data_key is valid
        if data_key not in self.data_map.keys():
            print("Error! No corresponding output value exists in data_map!")
            return
        if type(self.data_map[data_key][-1]) is not int and type(self.data_map[data_key][-1]) is not float:
            print("Error! Can only autoregChangedInput() for numeric data...")
            return

        #Loop through the data_map[time_key] data in REVERSE to get last data first
        points = 0
        change_loc = len(self.change_time)-1
        value_sum = 0
        time_index = len(self.data_map[self.time_key])-1
        has_calc = False
        #Initialize the list of inlet conditions (because we fill it in backwards)
        avg_list = [0.0]*len(self.change_time)
        for time in reversed(self.data_map[self.time_key]):
            if points >= avg_points:
                #Run a calculation here
                if has_calc == False:
                    value_sum = value_sum/avg_points
                    has_calc = True
                    if value_sum < 0 and non_neg == True:
                        value_sum = 0
                    avg_list[change_loc] = value_sum
                value_sum = 0
                #Update only if we are in next time bin
                if self.change_time[change_loc] >= time:
                    change_loc = change_loc - 1
                    #this is always the first point to grab, unless it is the first iteration
                    value_sum += self.data_map[data_key][time_index]
                    has_calc = False
                    points = 1
            else:
                #grab more data
                value_sum += self.data_map[data_key][time_index]
                points+=1
            time_index = time_index - 1
        #End time loop
        self.registerChangedInput(data_key, avg_list)

    ##This function will create a new column in the data_map by creating a step-wise set of
    #   input data based on the change_time and corresponding input_change information.
    #   When calling this function, it is unnecessary to call registerChangedInput as
    #   this function will automatically perform the associated actions of that function.
    #
    #       NOTE: data_key can be a single column or a list of columns
    def createStepChangeInputData(self, data_key, avg_points = 10, non_neg = True):
        if type(data_key) is not list:
            self.autoregChangedInput(data_key,avg_points,non_neg)
        else:
            for key in data_key:
                self.autoregChangedInput(key,avg_points,non_neg)
        #Iterate through the input_change map
        for new_key in self.input_change:
            i=0
            self.data_map[new_key] = []
            for time in self.data_map[self.time_key]:
                try:
                    time_limit = self.change_time[i+1]
                except:
                    time_limit = self.change_time[-1]+time
                if time > time_limit:
                    i+=1
                self.data_map[new_key].append(self.input_change[new_key][i])

    ##This function calculates a simple integral of a given column over the time range
    #
    #   Integral is calculated via the trapezoid rule and the result of the integral
    #   is returned as a single value. If the column does not contain numeric data,
    #   then no computation is performed.
    #
    #   Integral sums are computed as follows...
    #
    #   integrate( column, min, max) ==> integral(a,b)  [ f(t)*dt ]   \n
    #   sum += 0.5*[f(t+dt)+f(t)]*dt
    def calculateIntegralSum(self, column_name, min_time=None, max_time=None):
        sum = 0
        if column_name not in self.data_map.keys():
            print("Error! No corresponding inlet column exists in data_map!")
            return

        if min_time==None:
            min_time = self.data_map[self.time_key][0]
        if max_time==None:
            max_time = self.data_map[self.time_key][-1]
        time_set = self.extractRows(min_time, max_time)
        time_old = time_set[self.time_key][0]
        f_old = time_set[column_name][0]
        if type(f_old) is not int and type(f_old) is not float:
            print("Error! Cannot integrate non-numeric data!")
            return sum
        i=1
        while i<len(time_set[self.time_key]):
            time_new = time_set[self.time_key][i]
            f_new = time_set[column_name][i]
            sum += 0.5*(f_old+f_new)*(time_new-time_old)
            f_old = time_new
            f_old = f_new
            i+=1

        return sum


    ##This function will perform a trapezoid rule integration between two given curves
    #   in the data_map versus the time_key set of data. The first value of the integrated
    #   curve is always assumed to be zero. The units for the given columns do not matter,
    #   resulting curve will have same units as the given columns for inlet and outlet. Generally,
    #   this function is used to create a data column for Mass Retained in the catalyst.
    #
    #   User may pass an additional argument to determine whether or not they want the
    #   integral to be normalized. When normalized, the resulting column becomes unitless
    #   and is normalized to the magnitude of the maximum integrated value.
    #
    #   In addition to normalization, user may also provide a conversion factor to the
    #   calculated integral. The conversion factor can be used to scaled the normalized
    #   curve to a desired maximum or minimum, or can be used as a way to convert the
    #   units of the result from it's starting units to whatever the user desires.
    #
    #   For instance, if the units of the given column are in ppmv, but you want the result
    #   to come out to mol/L, then your conversion factor would be...
    #
    #       conv_factor = (1/10^6)*P/8.314/T
    #
    #                   where P is total pressure in kPa and T is temperature in K
    #
    #   As another unit conversion example, let's say we want the result to come out
    #   in typical adsorption units: mol adsorbed / L catalyst. Then, the conversion
    #   factor would be like above, but we would also multiple by the total system
    #   volume and divide by the solid volume of the catalyst.
    #
    #       conv_factor = (1/10^6)*P/8.314/T*(V_tot/V_cat)
    #
    #   The following relationship is assumed...
    #
    #   d(MR)/dt = Q*(Min - Mout)
    #
    #   Min = Mass in (given data column to represent inlet mass)
    #   Mout = Mass out (given data column to represent outlet mass)
    #   Q = flow rate (usually as space velocity [ hr^-1 ])
    #   MR = Mass retained in the catalyst (Representative of adsorbed mass)
    def calculateRetentionIntegral(self, inlet_column, outlet_column, normalized = False, conv_factor = 1):
        if inlet_column not in self.data_map.keys():
            print("Error! No corresponding inlet column exists in data_map!")
            return
        if outlet_column not in self.data_map.keys():
            print("Error! No corresponding outlet column exists in data_map!")
            return
        if self.have_flow_rate == False:
            print("Error! Cannot calculate retention integral without a flow rate. Flow rate is expected in the file name in kilo-volumes per hour...")
            return

        if normalized == True:
            ret_key = outlet_column+"-Retained (normalized)"
        else:
            ret_key = outlet_column+"-Retained"
        self.data_map[ret_key] = []
        self.data_map[ret_key].append(0)
        MR_old = 0
        max_value = abs(MR_old)
        time_old = self.data_map[self.time_key][0]
        Min_old = self.data_map[inlet_column][0]
        Mout_old = self.data_map[outlet_column][0]
        i=1
        while i<len(self.data_map[self.time_key]):
            time_new = self.data_map[self.time_key][i]
            Min_new = self.data_map[inlet_column][i]
            Mout_new = self.data_map[outlet_column][i]
            MR_new = MR_old + (time_new-time_old)/60*self.flow_rate*( (Min_old+Min_new)/2 - (Mout_old+Mout_new)/2 )
            if abs(MR_new) > max_value:
                max_value = abs(MR_new)
            self.data_map[ret_key].append(MR_new*conv_factor)
            time_old = time_new
            Min_old = Min_new
            Mout_old = Mout_new
            MR_old = MR_new
            i+=1

        #Loop one last time to normalize the integrated curve
        if normalized == True:
            i=0
            for value in self.data_map[ret_key]:
                self.data_map[ret_key][i] = self.data_map[ret_key][i]/max_value
                i+=1

    ## Function to fit a 2-peak distribution to the TPD
    #
    #   This function will attempt to fit a 2-peak normal distribution to the last time_frame set of
    #   data for the given column_name. We can use the optimized parameters to then determine how
    #   much of the TPD can be contributed to low temperature binding and high temperature binding.
    #
    #   Options:
    #
    #       - column_name: name of the column to fit the 2-peak normal distribution to
    #
    #       - display: if True, the images will be displayed once complete
    #
    #       - save: if True, the images will be saved to a file
    #
    #       - file_name: name of the file to save the plot to
    #
    #       - file_type: type of image file to save as (default = .png)
    #                       allowed types: .png, .pdf, .ps, .eps and .svg
    def fit2peakTPD(self, column_name, display=False, save=True, file_name="",file_type=".png",subdir="", p0 = []):
        if column_name not in self.data_map.keys():
            print("Error! No corresponding column exists in data_map!")
            return
        if file_type != ".png" and file_type != ".pdf" and file_type != ".ps" and file_type != ".eps" and file_type != ".svg":
            print("Warning! Unsupported image file type...")
            print("\tDefaulting to .png")
            file_type = ".png"
        #Check to see if folder exists and create if needed
        if subdir != "" and not os.path.exists(subdir) and save == True:
            os.makedirs(subdir)
            subdir+="/"
        xdata = self.extractRows(self.getTimeFrames()[-1][0], self.getTimeFrames()[-1][1])[self.time_key]
        ydata = self.extractRows(self.getTimeFrames()[-1][0], self.getTimeFrames()[-1][1])[column_name]

        if len(p0) != 6:
            p0 = [mean(xdata)-10,10,mean(ydata)*2,mean(xdata)+10,10,mean(ydata)*0.5]
        if p0[0] < xdata[0]:
            p0[0] = xdata[0]+10
        if p0[0] > xdata[-1]:
            p0[0] = xdata[-1]-10
        if p0[3] < xdata[0]:
            p0[3] = xdata[0]+10
        if p0[3] > xdata[-1]:
            p0[3] = xdata[-1]-10
        popt, pcov = curve_fit(double_peak_normal, xdata, ydata, p0, bounds=([xdata[0],5,0,xdata[0],5,0],[xdata[-1],15,10000,xdata[-1],15,10000]) )
        ymodel = []
        y1 = []
        y2 = []
        leg = []
        for time in xdata:
            ymodel.append(double_peak_normal(time, popt[0],popt[1],popt[2],popt[3],popt[4],popt[5]))
            if popt[0] < popt[3]:
                y1.append( normal_sum_func(time,1,[[popt[0],popt[1],popt[2]]] ) )
                y2.append( normal_sum_func(time,1,[[popt[3],popt[4],popt[5]]] ) )
            else:
                y2.append( normal_sum_func(time,1,[[popt[0],popt[1],popt[2]]] ) )
                y1.append( normal_sum_func(time,1,[[popt[3],popt[4],popt[5]]] ) )
        if popt[0] < popt[3]:
            ratio = popt[2]/popt[5]
        else:
            ratio = popt[5]/popt[2]
        fig = plt.figure()
        plt.plot(xdata,ydata)
        leg.append("TPD Data")
        plt.plot(xdata,ymodel,'r:')
        leg.append("TPD Model")
        plt.plot(xdata,y1,'--')
        leg.append("1st Peak")
        plt.plot(xdata,y2,'-.')
        leg.append("2nd Peak")
        plt.title("1st Peak -to- 2nd Peak Ratio = "+str(ratio))

        plt.legend(leg)
        plt.xlabel(self.time_key)
        plt.ylabel(column_name)
        plt.tight_layout()
        name = column_name+"--"+str(int(self.isothermal_temp))+"oC"

        if file_name == "":
            i=0
            for sec in name.split("/"):
                if i==0:
                    file_name += sec
                else:
                    file_name += "_per_" + sec
                i+=1
        if save == True:
            plt.savefig(subdir+file_name+'-TPDmodelPlot'+file_type)

            file = open(subdir+file_name+'-TPDcurve.dat','w')
            file.write(self.time_key+"\t")
            file.write(column_name+"\n")
            i=0
            running_sum = 0
            for time in xdata:
                running_sum += time-xdata[0]
                if running_sum >= 1.0:
                    file.write(str(time))
                    file.write("\t"+str(ydata[i]))
                    file.write("\n")
                    running_sum = 0
                i+=1
            file.close()

        if display == True:
            fig.show()
            print("\nDisplaying plot. Press enter to continue...(this closes the images)")
            input()
        plt.close()

        return popt, pcov



## PairedTransientData
# This is the object that can automatically pair bypass data with corresponding run data
#
# This object is used when you want to 'pair' inlet and outlet data sets together,
#    as well as perform some post-processing such as integrals over data sets. To initialize
#    the data set, you must pass a data file that contains the inlet data. For the
#    CLEERS data sets, an inlet data file is denoted by a "-bp" at the end of the file name
#    as opposed to an isothermal temperature.
class PairedTransientData(object):
    ## Constructor for the paired object requires the bypass and result file
    #
    # When creating an instance of this object, you must pass to files to the constructor
    #
    #   (i)     A file for the input data (or by-pass run data)         \n
    #   (ii)    A file for the output data (or non-by-pass run data)
    #
    # The constructor will check the file names to make sure that the given information aligns
    #   so that the given by-pass and non-by-pass data sets should actually be paired.
    #   Because of this check, maintaining the same file name conventions as before is necessary.
    #
    # @param bypass_file name of the bypass file
    # @param result_file name of the data run file that needs to be paired with the bypass file
    def __init__(self, bypass_file, result_file):

        # The constructor for the objects will automatically read the files
        ## object for bypass data
        self.bypass_trans_obj = TransientData(bypass_file)
        ## object for result data
        self.result_trans_obj = TransientData(result_file)
        self.material_name = self.result_trans_obj.material_name
        self.aging_time = self.result_trans_obj.aging_time
        self.aging_temp = self.result_trans_obj.aging_temp
        self.flow_rate = self.result_trans_obj.flow_rate
        self.aging_condition = self.result_trans_obj.aging_condition
        self.isothermal_temp = self.result_trans_obj.isothermal_temp
        self.run_type = self.result_trans_obj.run_type
        self.time_key = self.result_trans_obj.time_key
        ## Flag used to determine whether or not the data sets are aligned in time
        self.aligned = False

        self.material_name = self.result_trans_obj.material_name

        # Check some specific file information to make sure there are no errors
        self.file_errors = False
        if self.bypass_trans_obj.inlet_data == False:
            print("Error! Given by-pass data file is not recognized as by-pass data from the given file name...")
            self.file_errors = True
        if self.result_trans_obj.inlet_data == True:
            print("Error! Given results data file is not recognized as results data from the given file name...")
            self.file_errors = True
        if self.bypass_trans_obj.material_name != self.result_trans_obj.material_name:
            print("Error! Files given indicate they are for 2 different materials. They cannot be paired...")
            self.file_errors = True
        if self.bypass_trans_obj.aging_condition != self.result_trans_obj.aging_condition:
            print("Error! Files given indicate they are for 2 different aging conditions. They cannot be paired...")
            self.file_errors = True
        if self.bypass_trans_obj.flow_rate != self.result_trans_obj.flow_rate:
            print("Error! Files given indicate they are for 2 different flow rates. They cannot be paired...")
            self.file_errors = True

        #Check to make sure that the column names are the same for the by-pass and results data
        for item in self.bypass_trans_obj.data_map:
            if item not in self.result_trans_obj.data_map:
                print("Error! By-pass file and Results file must have all the same columns and column names...")
                self.file_errors = True
                break

    ##Function to print file information message to console
    def __str__(self):
        message = "\nBy-pass Information\n"
        message += "-------------------\n"
        message += str(self.bypass_trans_obj)
        message += "\nResults Information\n"
        message += "-------------------\n"
        message += str(self.result_trans_obj)
        if self.aligned == False:
            message += "\n\tWARNING: Data sets are unaligned currently!\n\t\tRun alignData() before processing information...\n"
        return message

    ##Function to display the column names to console
    def displayColumnNames(self):
        self.result_trans_obj.displayColumnNames()

    ##Function to compress columns in both data sets
    def compressColumns(self):
        self.bypass_trans_obj.compressColumns()
        self.result_trans_obj.compressColumns()

    ##Function to append a column to by-pass data
    def appendBypassColumn(self, column_name, data_set):
        self.bypass_trans_obj.appendColumn(column_name, data_set)

    ##Function to append a column to result data
    def appendResultColumn(self, column_name, data_set):
        self.result_trans_obj.appendColumn(column_name, data_set)

    ##Function to call the appendColumnByFrame function
    def appendColumnByFrame(self, column_name, column_value_list):
        if self.aligned == False:
            print("Error! Data must be aligned before performing any data processing...")
            return
        self.result_trans_obj.appendColumnByFrame(column_name, column_value_list)

    ##Function to extract a map of columns from the by-pass data
    def extractBypassColumns(self, column_list):
        return self.bypass_trans_obj.extractColumns(column_list)

    ##Function to extract a map of columns from the result data
    def extractResultColumns(self, column_list):
        return self.result_trans_obj.extractColumns(column_list)

    ##Function to extract a set of rows from the by-pass data
    def extractBypassRows(self, min_time, max_time):
        return self.bypass_trans_obj.extractRows(min_time,max_time)

    ##Function to extract a set of rows from the result data
    def extractResultRows(self, min_time, max_time):
        return self.result_trans_obj.extractRows(min_time,max_time)

    ##Function to extract result rows implicitly
    def extractRows(self, min_time, max_time):
        return self.result_trans_obj.extractRows(min_time,max_time)

    ##Function to get a data point from the by-pass data
    def getBypassDataPoint(self, time_value, column_name):
        return self.bypass_trans_obj.getDataPoint(time_value, column_name)

    ##Function to get a data point from the result data
    def getResultDataPoint(self, time_value, column_name):
        return self.result_trans_obj.getDataPoint(time_value, column_name)

    ##Function to get the maximum value of a column (should only use after calling alignData())
    def getMaximum(self, column_name):
        if self.aligned == False:
            print("Error! Data must be aligned before performing any data processing...")
            return
        return self.result_trans_obj.getMaximum(column_name)

    ##Function to get the minimum value of a column (should only use after calling alignData())
    def getMinimum(self, column_name):
        if self.aligned == False:
            print("Error! Data must be aligned before performing any data processing...")
            return
        return self.result_trans_obj.getMinimum(column_name)

    ##Function to get the average value of a column (should only use after calling alignData())
    def getAverage(self, column_name):
        if self.aligned == False:
            print("Error! Data must be aligned before performing any data processing...")
            return
        return self.result_trans_obj.getAverage(column_name)

    ##Function to get the range of values of a column (should only use after calling alignData())
    def getDataRange(self, column_name):
        if self.aligned == False:
            print("Error! Data must be aligned before performing any data processing...")
            return
        return self.result_trans_obj.getDataRange(column_name)

    ##Function to return the list of time ranges
    def getTimeFrames(self):
        return self.result_trans_obj.time_frames

    ##Function to return the number of rows
    def getNumRows(self):
        return self.result_trans_obj.num_rows

    ##Function to return the number of columns
    def getNumCols(self):
        return len(self.result_trans_obj.data_map)

    ##Function to delete a set of columns from both data sets
    def deleteColumns(self, column_list):
        self.bypass_trans_obj.deleteColumns(column_list)
        self.result_trans_obj.deleteColumns(column_list)

    ##Function to delete a set all columns from both data sets, except for the specified set
    def retainOnlyColumns(self, column_list):
        self.bypass_trans_obj.retainOnlyColumns(column_list)
        self.result_trans_obj.retainOnlyColumns(column_list)

    ##Function to align the bypass and results data so each has the same number of rows at the appropriate time values
    #
    #   Data is aligned such that the file that contains the most points in time is considered the 'master_set'.
    #   In most cases, the result data is the 'master_set' and we are trying to align the bypass data to it.
    #   Alignment is done on each time frame within the sets. This function is REQUIRED prior to performing
    #   any processing actions (except for compressColumns(), deleteColumns(), and retainOnlyColumns()).
    #   Object contains a flag variable to denote whether or not data has been aligned.
    #
    #   RECALL:
    #
    #       Time frames are denoted in each time series data file by a repeat of the column names. The time
    #       stamp values at which those repeated column names occur marks the end of the previous time frame.
    #       The bypass file and result file MUST have the same number of time frames in order to align the data.
    #
    #   NOTE:
    #
    #   This function is riddled with comment lines and print statements that are used for debugging
    #       DO NOT REMOVE ANY COMMENTS UNLESS THE FUNCTION IS FULLY TESTED AND APPROVED (still developing right now)
    #           This function is exceedingly complicated, do not modify unless you know what you are doing
    #
    #   NOTE 2:
    #
    #       Data in the bypass file is always misaligned in the x-axis, but may also be
    #       misaligned in the y-axis. This was demonstrated in the results data for
    #       NH3 storage at 350 oC where the bypass ppmv measurements were as high as
    #       50 ppmv above the outlet measurements during the actual run. This causes
    #       significant errors when trying to interpret mass retention data. To fix
    #       this issue, data will also be aligned in the y-axis by comparing the
    #       autoregChangedInput() for each column for both bypass and results and
    #       using that information to scale the y-axis bypass information to match
    #       the expected outlet information from each data run.
    #
    # @param addNoise if True, then the gaps in data are filled in with random noise to simulate the missing information \n
    #                 if False, then the gaps in data are filled in with the average value of the last few points of the
    #                           current time frame.
    # @param verticalAlignment if True, then the y-axis data values from the bypass run are scaled to be the same as \n
    #                           the non-bypass data. If False, no scaling is applied to bypass data.
    def alignData(self, addNoise = True, verticalAlignment = True):
        if self.aligned == True:
            print("Data already aligned. Cannot re-align...")
            return

        for item in self.bypass_trans_obj.data_map:
            if item != self.bypass_trans_obj.time_key:
                if type(self.bypass_trans_obj.data_map[item][-1]) is int or type(self.bypass_trans_obj.data_map[item][-1]) is float:
                    self.bypass_trans_obj.autoregChangedInput(item,10,False)
        for item in self.result_trans_obj.data_map:
            if item != self.result_trans_obj.time_key:
                if type(self.result_trans_obj.data_map[item][-1]) is int or type(self.result_trans_obj.data_map[item][-1]) is float:
                    self.result_trans_obj.autoregChangedInput(item,10,False)
        #Check to make sure all input_change maps have the same keys and throw out any dissimilar keys
        dissimilar_key = []
        for change in self.bypass_trans_obj.input_change:
            if change not in self.result_trans_obj.input_change.keys():
                dissimilar_key.append(change)
        for diff_key in dissimilar_key:
            if diff_key in self.bypass_trans_obj.input_change.keys():
                del self.bypass_trans_obj.input_change[diff_key]
            if diff_key in self.result_trans_obj.input_change.keys():
                del self.result_trans_obj.input_change[diff_key]
        frame_ratios = {}
        for item in self.bypass_trans_obj.input_change:
            frame_ratios[item] = []
            i=0
            for value in self.bypass_trans_obj.input_change[item]:
                if abs(self.result_trans_obj.input_change[item][i]-self.bypass_trans_obj.input_change[item][i]) > 0.01:
                    if abs(self.bypass_trans_obj.input_change[item][i]) > 2.0*math.sqrt(sys.float_info.epsilon):
                        if verticalAlignment == True:
                            frame_ratios[item].append(self.result_trans_obj.input_change[item][i]/self.bypass_trans_obj.input_change[item][i])
                        else:
                            frame_ratios[item].append(1)
                    else:
                        frame_ratios[item].append(1)
                else:
                    frame_ratios[item].append(1)
                i+=1

        #Create a set of frame_ratio_tuples to get a list of time frames for which the frame_ratios will apply
        frame_ratio_tuples = []
        for i in range(1,len(self.result_trans_obj.change_time)):
            frame_ratio_tuples.append((self.result_trans_obj.change_time[i-1],self.result_trans_obj.change_time[i]))
        frame_ratio_tuples.append( (self.result_trans_obj.change_time[-1],self.result_trans_obj.data_map[self.result_trans_obj.time_key][-1]) )


        # Identify which data set will be considered the 'master_set'
        #   Master set will be the data set that has the most data available
        #   The sub-set will be forced to conform to the master set
        #
        # Our goal is to expand the data in the sub-set to match the master set through
        #   linear interpolations or other means. Thus, the master set of data won't be
        #   changed, only the sub-set of data will change.
        master_set = {}             #Point to master data
        sub_set = {}                #Point to non-master data
        new_set = {}                #New map to make to override non-master data when finished
        master_change_time = []     #Point to list of master change_time
        sub_change_time = []        #Point to list of non-master change_time
        isResultMaster = False      #True if result_trans_obj is master, False if bypass_trans_obj is master
        time_column = self.bypass_trans_obj.time_key
        # Find master and point master's data_map to master_set
        #       This is so we only have to write code to iterate on master set
        #       Do not copy, only point. Master set will not change, so a pointer works best
        #
        # Do same with sub_set, point sub_set to the non-master data set
        #       However, we also need a new data set to create. That new data set
        #       will eventually override the non-master original data
        if len(self.bypass_trans_obj.data_map[time_column]) > len(self.result_trans_obj.data_map[time_column]):
            isResultMaster = False
            master_set = self.bypass_trans_obj.data_map
            for i in range(1,len(self.bypass_trans_obj.change_time)):
                master_change_time.append((self.bypass_trans_obj.change_time[i-1],self.bypass_trans_obj.change_time[i]))
            master_change_time.append((self.bypass_trans_obj.change_time[-1],self.bypass_trans_obj.data_map[time_column][-1]))

            sub_set = self.result_trans_obj.data_map
            for i in range(1,len(self.result_trans_obj.change_time)):
                sub_change_time.append((self.result_trans_obj.change_time[i-1],self.result_trans_obj.change_time[i]))
            sub_change_time.append((self.result_trans_obj.change_time[-1],self.result_trans_obj.data_map[time_column][-1]))
        else:
            isResultMaster = True
            master_set = self.result_trans_obj.data_map
            for i in range(1,len(self.result_trans_obj.change_time)):
                master_change_time.append((self.result_trans_obj.change_time[i-1],self.result_trans_obj.change_time[i]))
            master_change_time.append((self.result_trans_obj.change_time[-1],self.result_trans_obj.data_map[time_column][-1]))

            sub_set = self.bypass_trans_obj.data_map
            for i in range(1,len(self.bypass_trans_obj.change_time)):
                sub_change_time.append((self.bypass_trans_obj.change_time[i-1],self.bypass_trans_obj.change_time[i]))
            sub_change_time.append((self.bypass_trans_obj.change_time[-1],self.bypass_trans_obj.data_map[time_column][-1]))

        #Check to make sure that the change times are already aligned
        if len(master_change_time) != len(sub_change_time):
            print("Error! Cannot align data if the number of 'change_time' instances are not the same!")
            return

        #Initialize the new_set with same number of columns and column names,
        #   but with the number of data points matching master_set
        for item in sub_set:
            #Check the data type of the master list at the given item and initialize new_set accordingly
            if type(master_set[item][0]) is not int and type(master_set[item][0]) is not float:
                new_set[item] = [""]*len(master_set[item])
            else:
                new_set[item] = [0.0]*len(master_set[item])

        #First data point in the new_set should auto-align to the first master_set time,
        #   regardless of discrepencies in the first time point. This is because
        #   we have no other information yet to perform any necessary interpolation.
        for item in new_set:
            new_set[item][0] = sub_set[item][0]
        new_set[time_column][0] = master_set[time_column][0]

        #Data from sub_set should be either compressed or elongated depending on the change_times
        '''
        i=0
        for tuple in sub_change_time:
            sub_dist = tuple[1]-tuple[0]
            master_dist = master_change_time[i][1] - master_change_time[i][0]
            if master_dist >= sub_dist:
                print("Elongate Sub (or keep same)")
            else:
                print("Compress Sub")
            i+=1
        '''

        #Want to iterate through all rows in the new data set
        print("\nData Alignment has started... Please wait...")
        tup_i = 0       #Index for tuple frame
        i = 1           #Index for master row
        j = 1           #Index for sub row
        #The time shifted values below represent how far in time we have traversed for the given tuple frame
        master_time_shifted = 0     #Start with no time shift for first reference frame
        sub_time_shifted = 0        #Same reason as above
        points_in_frame = 0         #Counter to keep track of the total number of points in the sub_set
                                    #   that exist in the current master_frame
        new_frame = False
        avg = {}
        std = {}
        hasCalcMean = False
        while i < len(new_set[time_column]):
            #Enfore matching time values
            new_set[time_column][i] = master_set[time_column][i]

            #Find when to update the tuple frame
            if master_set[time_column][i] > master_change_time[tup_i][1]:
                # ------------ Update tuple frame -------------
                new_frame = True
                tup_i +=1
                #When the tuple frame updates, we are now forced to update j
                #   j gets updated to the first indexed time value within the tuple frame
                for n in range(j,len(sub_set[time_column])):
                    if sub_change_time[tup_i][0] <= sub_set[time_column][n]:
                        j = n+1
                        break

                #At this point, i and j are aligned to their own tuple frames
                #       However, each may be misaligned with each other
                #Each time we move into a tuple frame, we should use a time_shited
                #       value for the actual time to represent distance traveled in
                #       the given frame of reference.
                #This resets the time shifting at each tuple frame update
                master_time_shifted = master_set[time_column][i-1] - master_change_time[tup_i][0]
                sub_time_shifted = sub_set[time_column][j-1] - sub_change_time[tup_i][0]
            else:
                new_frame = False
            #END Update for tuple frame

            #Calculate distance in time of our position in the master set of data from the previous point
            master_dt = master_set[time_column][i] - master_set[time_column][i-1]
            #Calculate the master time within the frame (not the actual time value)
            master_time_shifted += master_dt

            #Calculate target time for sub_set based on time travel and current tuple frame
            target_time = sub_time_shifted + master_time_shifted
            #This line was needed for additional alignment because our first frame starts at 0, but first data starts  > 0
            if tup_i == 0:
                target_time += sub_set[time_column][j-1]

            #Check to see if target time for sub_set is outside of the correct tuple frame
            #   If it is outside of the frame, then extrapolate values
            if target_time > (sub_change_time[tup_i][1]-sub_change_time[tup_i][0]):
                # --------------- Target Out of Frame ------------------
                # When the target is out of the tuple frame, we need to supplement the data to
                #   fill in any gaps. Simplest way to fill the gap is just to repeat the last
                #   relevant data point from the previous frame. However, This is likely
                #   unrealistic and may produce error.
                #
                #   Other options to consider:
                #       (i) Grab the last point and extend it to the next frame
                #       (ii) Average the last few points of the current frame to use as an extension
                #       (iii) Use a running average with artifical "noise" added for realism
                for item in new_set:
                    if item is not time_column:
                        if type(new_set[item][i]) is not int and type(new_set[item][i]) is not float:
                            #If the data in the column is non-numeric, then just grab the prior time value
                            new_set[item][i] = new_set[item][i-1]
                        else:
                            #If the data in the column is numeric, then just grab the prior time value
                            #new_set[item][i] = new_set[item][i-1]

                            #If the data in the column is numeric, then just grab average the last 1/4 of elements
                            if addNoise == False:
                                new_set[item][i] = new_set[item][i-1]
                                if hasCalcMean == False:
                                    avg[item] = mean(new_set[item][i-int(points_in_frame/4):i])
                                new_set[item][i] = avg[item]
                            #If the data in the column is numeric, then average the last few points and
                            #   and in some random noise based on the standard deviation of the last few points
                            else:
                                new_set[item][i] = new_set[item][i-1]
                                if hasCalcMean == False:
                                    avg[item] = mean(new_set[item][i-int(points_in_frame/4):i])
                                    std[item] = stdev(new_set[item][i-int(points_in_frame/4):i])
                                new_set[item][i] = random.gauss(avg[item],std[item])
                    else:
                        new_set[item][i] = master_set[item][i]
                hasCalcMean = True
            #   If it is inside of the frame, then interpolate or extract values
            else:
                #----------------- Data Mapping ---------------------
                #master_set[time_column][i]  ---> target_time+sub_change_time[tup_i][0]
                # Current time in master set maps to the target time in the sub_set
                #       plus the off-set dictated by the tuple frame of the sub_set
                hasCalcMean = False
                if new_frame == False:
                    points_in_frame += 1
                else:
                    points_in_frame = 1
                for item in new_set:
                    # Perform a linear interpolation for the correct data_map and place into the new_set
                    if item is not time_column:
                        if isResultMaster == True:
                            new_set[item][i] = self.bypass_trans_obj.getDataPoint(target_time+sub_change_time[tup_i][0],item)
                        else:
                            new_set[item][i] = self.result_trans_obj.getDataPoint(target_time+sub_change_time[tup_i][0],item)
                    else:
                        new_set[item][i] = master_set[item][i]
            i+=1
        #END while loop through all times

        #At this point, the new_set should contain all of the data for the sub_set that needs alignment
        #   Delete all existing data in the sub_set and replace with the new_set
        for item in new_set:
            del sub_set[item]
        for item in new_set:
            sub_set[item] = []
            for value in new_set[item]:
                sub_set[item].append(value)

        #If we made it this far without error, then the data should be aligned (fingers crossed, no jynx)
        print("Complete!")
        self.aligned = True
        if isResultMaster == True:
            self.bypass_trans_obj.num_rows = len(sub_set[time_column])
        else:
            self.result_trans_obj.num_rows = len(sub_set[time_column])

        #Lastly, for book-keeping purposes, point all results columns to the corresponding bypass
        #       columns by appending the columns with the appropriate column names
        #After this method has been called, result and bypass structures no longer
        #       have identical numbers of columns and column names.
        new_name_set = []
        for item in self.bypass_trans_obj.data_map:
            if item != self.bypass_trans_obj.time_key:
                appendName = item+"[bypass]"
                new_name_set.append(appendName)
                self.result_trans_obj.appendColumn(appendName, self.bypass_trans_obj.data_map[item])

        #Before we end here, the data that was just appened to the result_trans_obj needs to be iterated through
        #       one more time in order to scale all the new data according to the frame_ratios. This
        #       must also make use of the time_key for result_trans_obj data since the frame_ratios
        #       only apply to specific time frames within the new data set.
        for new_name in new_name_set:
            frame_key = new_name.split("[bypass]")[0]+"[input]"
            #Only apply the ratio adjustment to the new data that has a corresponding frame key
            if frame_key in frame_ratios.keys():
                #iterate through the rows of data
                tuple_index = 0
                i=0
                for time in self.result_trans_obj.data_map[self.result_trans_obj.time_key]:
                    #Check to see if tuple_index should update
                    if time > frame_ratio_tuples[tuple_index][1]:
                        tuple_index +=1

                    #use the tuple_index as the index for the frame_ratios
                    self.result_trans_obj.data_map[new_name][i] = self.result_trans_obj.data_map[new_name][i]*frame_ratios[frame_key][tuple_index]
                    i+=1


    #End alignData()


    ##Function will compress the rows of data based on the given compression factor
    #
    # NOTE: Before you can compress the rows of data, each data set must be aligned
    def compressRows(self, factor = 2):
        if self.aligned == False:
            print("Error! Cannot compress the rows of data until data is aligned! Run alignData() first...")
            return

        self.bypass_trans_obj.compressRows(factor)
        self.result_trans_obj.compressRows(factor)

    ##This function calculates a simple integral of a given column over the time range
    #
    #   Integral is calculated via the trapezoid rule and the result of the integral
    #   is returned as a single value. If the column does not contain numeric data,
    #   then no computation is performed.
    #
    #   Integral sums are computed as follows...
    #
    #   integrate( column, min, max) ==> integral(a,b)  [ f(t)*dt ]   \n
    #   sum += 0.5*[f(t+dt)+f(t)]*dt
    def calculateIntegralSum(self, column_name, min_time=None, max_time=None):
        return self.result_trans_obj.calculateIntegralSum(column_name, min_time, max_time)

    ##Function will compute a mass retention integral for the given column
    #
    #   The align data function must have already been called. That function
    #   ensures the data sets for bypass and results are aligned and appends the
    #   aligned data from bypass to the results with "[bypass]" added to the
    #   end of the file name.
    #
    #   By default, the calculated integral will have the same units as the data
    #       in the given column, however, the user may specify to have the integral
    #       normalized (thus making the result unitless) and/or may specify a unit
    #       conversion factor to multiple the results by in order to get a specific
    #       unit outcome.
    #
    #   NOTE:
    #
    #           As an option, the user may specify an input_col_name if that name is
    #           expected to be different from the given column_name suffixed with
    #           "[bypass]". May be useful if creating new columns by performing
    #           unit conversions or other mathOperation() functions.
    def calculateRetentionIntegral(self, column_name, normalized = False, conv_factor = 1, input_col_name=""):
        #Check for alignment
        if self.aligned == False:
            print("Error! Data must be aligned first! Call alignData()...")
            return
        appendName = column_name+"[bypass]"
        if input_col_name != "":
            appendName = input_col_name
        if appendName not in self.result_trans_obj.data_map.keys():
            print("Error! The appended column_name does not match any by-pass/result data keys!")
            return
        if column_name not in self.result_trans_obj.data_map.keys():
            print("Error! The column_name does not match any result data keys!")
            return
        self.result_trans_obj.calculateRetentionIntegral(appendName, column_name, normalized, conv_factor)

    ## This function is used to perform a number of operations using a given column
    #
    #   The default setting is to operate on the given column to change it's values,
    #   however, the user can specify that the result should create a new column to
    #   append to the map if desired.
    #
    #   Supported Operations:
    #
    #       multiply by a scalar ==> this_column, "*", constant
    #
    #       divide by a scalar   ==> this_column, "/", constant
    #
    #       add a scalar         ==> this_column, "+", constant
    #
    #       subtract a scalar    ==> this_column, "-", constant
    #
    #       multiply by a column ==> this_column, "*", other_column
    #
    #       divide by a column   ==> this_column, "/", other_column
    #
    #       add a column         ==> this_column, "+", other_column
    #
    #       subtract a column    ==> this_column, "-", other_column
    #
    #   NOTE:
    #       `
    #           Multiplying and dividing by a given column will just take all
    #           corresponding rows of the first column and perform the operation
    #           using the data in the other corresponding row of the other column.
    #
    #   EXAMPLE USAGE:
    #
    #       obj.mathOperation("A","*","B")
    #
    #               this will compute the result of the multiplication of every
    #                   row of A times every row of B and store the result in A
    #
    #       obj.mathOperation("A","+",273,True)
    #
    #               this will compute the result of every row of A plus 273 and
    #                   store the result in a new column in the map named "A+273"
    #
    #       obj.mathOperation("A","/","B",True,"Res")
    #
    #               this will compute the result of the division of every row
    #                   of A divided by every row of B and store the result in a
    #                   new column in the map named "Res"
    def mathOperation(self, column_name, operator, value_or_column, append_new = False, append_name = ""):
        if self.aligned == False:
            print("Error! Data must be aligned BEFORE performing operations! Call alignData() first...")
            return
        self.result_trans_obj.mathOperation(column_name, operator, value_or_column, append_new, append_name)

    ##Function will print out the results to an sinle output file
    #
    #   Data must already be aligned. This allows us to only print out information
    #   in the results object, since after alignment the results object is linked
    #   to the columns in the bypass object.
    def printAlltoFile(self, file_name = ""):
        if self.aligned == False:
            print("Error! Data must be aligned first! Call alignData()...")
            print("\t\t(NOTE: if you want to print out the bypass and result data separately,")
            print("\t\t       then call each object's s respective printAlltoFile() functions)")
            return
        if file_name == "":
            file_name = self.result_trans_obj.input_file_name.split(".")[0]+"-AllPairedOutput.dat"
        self.result_trans_obj.printAlltoFile(file_name)

    ##This function is used to print out the equilibrium information in each column for each time frame
    def printEquilibriaTimeFrames(self, file_name = "", avg_points = 10):
        if self.aligned == False:
            print("Error! Data must be aligned first! Call alignData()...")
            print("\t\t(NOTE: if you want to print out the bypass and result data separately,")
            print("\t\t       then call each object's s respective printAlltoFile() functions)")
            return
        if file_name == "":
            file_name = self.result_trans_obj.input_file_name.split(".")[0]+"-AllPairedEquilibria.dat"
        self.result_trans_obj.printEquilibriaTimeFrames(file_name,avg_points)

    ##Function to print only specific columns to an output file
    def printColumnstoFile(self, column_list, file_name = ""):
        if self.aligned == False:
            print("Error! Data must be aligned first! Call alignData()...")
            print("\t\t(NOTE: if you want to print out the bypass and result data separately,")
            print("\t\t       then call each object's s respective printAlltoFile() functions)")
            return
        if file_name == "":
            file_name = self.result_trans_obj.input_file_name.split(".")[0]+"-SelectedPairedOutput.dat"
        self.result_trans_obj.printAlltoFile(column_list, file_name)

    ##Function to a create plot from the data_map
    #
    #   Options:
    #
    #       - column_list: list of columns to create plots of (default is all columns of plottable data)
    #
    #       - range: tuple of the minimum to maximum time values that you want plotted (default is full range)
    #
    #       - display: if True, the images will be displayed once complete
    #
    #       - save: if True, the images will be saved to a file
    #
    #       - file_name: name of the file to save the plot to
    #
    #       - file_type: type of image file to save as (default = .png)
    #                       allowed types: .png, .pdf, .ps, .eps and .svg
    def createPlot(self, column_list = [], range=None, display=False, save=True, file_name="",file_type=".png",subdir=""):
        if self.aligned == False:
            print("Error! Data must be aligned first! Call alignData()...")
            print("\t\t(NOTE: if you want to create a plot of the bypass and result data separately,")
            print("\t\t       then call each object's s respective createPlot() functions)")
            return
        self.result_trans_obj.createPlot(column_list, range, display, save, file_name, file_type, subdir)

    ##Quick use function for saving all processed data plots in a series of output files
    #
    #   Function will automatically pair result data and bypass data together
    #   File names will be automatically generated and plots will not be displayed live
    def savePlots(self, range=None, folder="", file_type=".png"):
        bypass_list = []
        result_list = []
        for item in self.result_trans_obj.data_map:
            if "[bypass]" in item and item != self.result_trans_obj.time_key:
                bypass_list.append(item)
            else:
                if item != self.result_trans_obj.time_key:
                    result_list.append(item)
        pairs = []
        for item1 in bypass_list:
            for item2 in result_list:
                if item1.split("[bypass]")[0] in item2:
                    pairs.append([item1,item2])
        for pair in pairs:
            if pair[1] in result_list:
                result_list.remove(pair[1])

        #Print to a folder if possible
        if folder == "":
            if range != None:
                folder = self.result_trans_obj.input_file_name.split(".")[0]+"-range("+str(int(range[0]))+"-"+str(int(range[1]))+")Plots/"
            else:
                folder = self.result_trans_obj.input_file_name.split(".")[0]+"-range(All)"+"Plots/"
        #Check to see if folder exists and create if needed
        if not os.path.exists(folder):
            os.makedirs(folder)
        #Now, pairs has the list of paired columns to plot
        #   and result_list has the list of unpaired columns to plot
        for pair in pairs:
            self.createPlot(pair,range,False,True,"",file_type,folder)
        for solo in result_list:
            self.createPlot(solo,range,False,True,"",file_type,folder)

    ##Function to iteratively save all plots in all time frames separately
    def saveTimeFramePlots(self, folder="", file_type=".png"):
        #First, save the full time ranges
        self.savePlots(None,folder,file_type)
        #Then iterate through all time frames to save separately
        for frame in self.result_trans_obj.time_frames:
            self.savePlots(frame,folder,file_type)

    ## Function to fit a 2-peak distribution to the TPD
    #
    #   This function will attempt to fit a 2-peak normal distribution to the last time_frame set of
    #   data for the given column_name. We can use the optimized parameters to then determine how
    #   much of the TPD can be contributed to low temperature binding and high temperature binding.
    #
    #   Options:
    #
    #       - column_name: name of the column to fit the 2-peak normal distribution to
    #
    #       - display: if True, the images will be displayed once complete
    #
    #       - save: if True, the images will be saved to a file
    #
    #       - file_name: name of the file to save the plot to
    #
    #       - file_type: type of image file to save as (default = .png)
    #                       allowed types: .png, .pdf, .ps, .eps and .svg
    def fit2peakTPD(self, column_name, display=False, save=True, file_name="",file_type=".png",subdir="", p0 = []):
        return self.result_trans_obj.fit2peakTPD(column_name, display, save, file_name, file_type, subdir, p0)



## Function for testing the above objects
#Testing Paired data
def testing():
    #a = normal_sum_func(0,1,[[1,1,1]])
    #print(a)
    h2o_comp = False
    if h2o_comp == True:
        test05 = PairedTransientData("20160209-CLRK-BASFCuSSZ13-700C4h-NH3H2Ocomp-30k-0_2pctO2-11-3pctH2O-400ppmNH3-bp.dat","20160209-CLRK-BASFCuSSZ13-700C4h-NH3H2Ocomp-30k-0_2pctO2-11-3pctH2O-400ppmNH3-150C.dat")
    else:
        test05 = PairedTransientData("20160205-CLRK-BASFCuSSZ13-700C4h-NH3DesIsoTPD-30k-0_2pctO2-5pctH2O-bp.dat","20160205-CLRK-BASFCuSSZ13-700C4h-NH3DesIsoTPD-30k-0_2pctO2-5pctH2O-175C.dat")
        #test05 = PairedTransientData("20160205-CLRK-BASFCuSSZ13-700C4h-NH3DesIsoTPD-30k-0_2pctO2-5pctH2O-bp.dat","20160205-CLRK-BASFCuSSZ13-700C4h-NH3DesIsoTPD-30k-0_2pctO2-5pctH2O-350C.dat")
        #test05 = PairedTransientData("20160311-CLRK-BASFCuSSZ13-800C16h-NH3DesIsoTPD-30k-0_2pctO2-5pctH2O-bp.dat","20160311-CLRK-BASFCuSSZ13-800C16h-NH3DesIsoTPD-30k-0_2pctO2-5pctH2O-350C.dat")
    test05.compressColumns()
    #test05.displayColumnNames()

    #NOTE: If you are going to only retain specific columns, you should run that function
    #       prior to running alignData(). This saves significant computational effort.
    #test05.retainOnlyColumns(['Elapsed Time (min)','NH3 (300,3000)', 'H2O% (20)'])
    test05.retainOnlyColumns(['Elapsed Time (min)','NH3 (300,3000)', 'H2O% (20)', 'TC bot sample in (C)', 'TC bot sample mid 1 (C)', 'TC bot sample mid 2 (C)', 'TC bot sample out 1 (C)', 'TC bot sample out 2 (C)', 'P bottom in (bar)', 'P bottom out (bar)'])
    test05.alignData()

    #print("Calculating sum...")
    #print(test05.calculateIntegralSum('NH3 (300,3000)[bypass]',15,24))
    #print(test05.calculateIntegralSum('NH3 (300,3000)',15,24))
    #print(str(test05.calculateIntegralSum('NH3 (300,3000)[bypass]',15,24)-test05.calculateIntegralSum('NH3 (300,3000)',15,24)))

    print("\nFitting curves...")
    #p0 = [ 254.17897219,   13.89793735, 2636.78644076,  278.23501362,    6.78306823, 372.94330633]
    p0=[]
    popt, pcov = test05.fit2peakTPD('NH3 (300,3000)', p0=p0)
    #[ 254.17897219,   13.89793735, 2636.78644076,  278.23501362,    6.78306823, 372.94330633]
    print(popt)

    #NOTE: After data is aligned, all result and bypass data are now in the result_trans_obj.
    #The bypass columns have the same names, but will be suffixed with [bypass]
    #We can then use this knowledge to manipulate or delete very specific sets of data

    #Convert all temperatures from (C) to Kelvin, then delete old columns
    test05.mathOperation('TC bot sample in (C)',"+",273.15, True, 'TC bot sample in (K)')
    test05.deleteColumns('TC bot sample in (C)')
    test05.mathOperation('TC bot sample mid 1 (C)',"+",273.15, True, 'TC bot sample mid 1 (K)')
    test05.deleteColumns('TC bot sample mid 1 (C)')
    test05.mathOperation('TC bot sample mid 2 (C)',"+",273.15, True, 'TC bot sample mid 2 (K)')
    test05.deleteColumns('TC bot sample mid 2 (C)')
    test05.mathOperation('TC bot sample out 1 (C)',"+",273.15, True, 'TC bot sample out 1 (K)')
    test05.deleteColumns('TC bot sample out 1 (C)')
    test05.mathOperation('TC bot sample out 2 (C)',"+",273.15, True, 'TC bot sample out 2 (K)')
    test05.deleteColumns('TC bot sample out 2 (C)')

    #Delete the temperature columns from the bypass run that we don't need
    #NOTE: Will result in errors displayed, but only because of name changes (this is ok)
    test05.deleteColumns(['TC bot sample in (C)[bypass]','TC bot sample mid 1 (C)[bypass]','TC bot sample mid 2 (C)[bypass]','TC bot sample out 1 (C)[bypass]','TC bot sample out 2 (C)[bypass]'])

    #Now, convert all pressures from bar to kPa and delete the extra [bypass] columns
    test05.mathOperation('P bottom in (bar)',"*",100,True,'P bottom in (kPa)')
    test05.deleteColumns('P bottom in (bar)')
    test05.mathOperation('P bottom out (bar)',"*",100,True,'P bottom out (kPa)')
    test05.deleteColumns('P bottom out (bar)')
    test05.deleteColumns(['P bottom in (bar)[bypass]','P bottom out (bar)[bypass]'])

    #NOTE: Always perform data processing BEFORE compressing the rows!
    #normal = False  #If true, then the integral is normalized and unitless
                #Otherwise, the integral has same units as the given column
    #conv_factor = 4.30554723150288E-08
    #NOTE: This factor is for 101.35 kPa, 150 oC, and a 0.0157 L total volume with 0.33 void fraction
    #           Also, this is to convert from ppmv to mol adsorbed per L catalyst
    #test05.calculateRetentionIntegral('NH3 (300,3000)', normal, conv_factor)
    #test05.calculateRetentionIntegral('H2O% (20)', normal, 1)

    #Calculate the retention integral, then perform unit conversions
    test05.calculateRetentionIntegral('NH3 (300,3000)')
    test05.calculateRetentionIntegral('H2O% (20)')

    #NOTE: The retention integral will have the name of the given column suffixed
    #       with '-Retained' and will have the same units as the given column. Use
    #       that information to perform specific actions on that column.

    #NH3 has units of ppmv, want to convert this to mol adsorbed / L catalyst
    test05.mathOperation('NH3 (300,3000)-Retained',"/",1E6)                     #From ppmv to molefraction
    test05.mathOperation('NH3 (300,3000)-Retained',"*","P bottom in (kPa)")    #From molefraction to kPa
    test05.mathOperation('NH3 (300,3000)-Retained',"/",8.314)
    test05.mathOperation('NH3 (300,3000)-Retained',"/",'TC bot sample in (K)') #From kPa to mol/L using Ideal gas law
    test05.mathOperation('NH3 (300,3000)-Retained',"*",0.015708)                #From mol/L to total moles (multiply by total volume)
    #From total moles to mol ads / L cat using solids fraction, then store in new column and delete old column
    test05.mathOperation('NH3 (300,3000)-Retained',"/",(1-0.3309)*0.015708,True,"NH3 ads (mol/L)")
    test05.deleteColumns('NH3 (300,3000)-Retained')

    #H2O has units of %, want to convert this to mol adsorbed / L catalyst
    test05.mathOperation('H2O% (20)-Retained',"/",100)                     #From % to molefraction
    test05.mathOperation('H2O% (20)-Retained',"*","P bottom in (kPa)")    #From molefraction to kPa
    test05.mathOperation('H2O% (20)-Retained',"/",8.314)
    test05.mathOperation('H2O% (20)-Retained',"/",'TC bot sample in (K)') #From kPa to mol/L using Ideal gas law
    test05.mathOperation('H2O% (20)-Retained',"*",0.015708)                #From mol/L to total moles (multiply by total volume)
    #From total moles to mol ads / L cat using solids fraction, then store in new column and delete old column
    test05.mathOperation('H2O% (20)-Retained',"/",(1-0.3309)*0.015708,True,"H2O ads (mol/L)")
    test05.deleteColumns('H2O% (20)-Retained')


    #Instead of using a constant factor for unit conversion, this time we will
    #convert the units for the desired inlet and outlet columns first, then ask
    #for the integration of the converted unit columns. This may be the most
    #accurate approach since there are differences in the inlet and outlet
    #temperatures and pressures which are used to convert from molefractions
    #to molar concentrations.

    #   Convert NH3 for bypass to mol/L by first converting to molefraction and saving in a new column
    test05.mathOperation('NH3 (300,3000)[bypass]',"/",1E6,True,'NH3 (mol/L)[bypass]')
    #   Then convert that molefraction to kPa and override the new column previously created
    test05.mathOperation('NH3 (mol/L)[bypass]',"*",'P bottom in (kPa)')
    #   Perform the next set of conversion overrides to get to mol/L
    test05.mathOperation('NH3 (mol/L)[bypass]',"/",8.314)
    test05.mathOperation('NH3 (mol/L)[bypass]',"/",'TC bot sample in (K)')
    #   NOTE: we named the new column for the input with a suffixed "[bypass]"
    #           because the integration routine is designed to specifically
    #           look for the "[bypass]" suffix with matching prefixes

    #   Convert NH3 for outlet to mol/L by first converting to molefraction and saving in a new column
    test05.mathOperation('NH3 (300,3000)',"/",1E6,True,'NH3 (mol/L)')
    #   Then convert that molefraction to kPa and override the new column previously created
    test05.mathOperation('NH3 (mol/L)',"*",'P bottom in (kPa)')
    #   Perform the next set of conversion overrides to get to mol/L
    test05.mathOperation('NH3 (mol/L)',"/",8.314)
    test05.mathOperation('NH3 (mol/L)',"/",'TC bot sample in (K)')
    #   NOTE: we named the new column based on what the inlet column was previously
    #           named, but removed the suffixed "[bypass]"

    #   Convert H2O for bypass to mol/L by first converting to molefraction and saving in a new column
    test05.mathOperation('H2O% (20)[bypass]',"/",100,True,'H2O (mol/L)[bypass]')
    #   Then convert that molefraction to kPa and override the new column previously created
    test05.mathOperation('H2O (mol/L)[bypass]',"*",'P bottom in (kPa)')
    #   Perform the next set of conversion overrides to get to mol/L
    test05.mathOperation('H2O (mol/L)[bypass]',"/",8.314)
    test05.mathOperation('H2O (mol/L)[bypass]',"/",'TC bot sample in (K)')
    #   NOTE: we named the new column for the input with a suffixed "[bypass]"
    #           because the integration routine is designed to specifically
    #           look for the "[bypass]" suffix with matching prefixes

    #   Convert H2O for outlet to mol/L by first converting to molefraction and saving in a new column
    test05.mathOperation('H2O% (20)',"/",100,True,'H2O (mol/L)')
    #   Then convert that molefraction to kPa and override the new column previously created
    test05.mathOperation('H2O (mol/L)',"*",'P bottom in (kPa)')
    #   Perform the next set of conversion overrides to get to mol/L
    test05.mathOperation('H2O (mol/L)',"/",8.314)
    test05.mathOperation('H2O (mol/L)',"/",'TC bot sample in (K)')

    #The way this was done gives us 4 new columns containing the inlet and outlet
    #   data for both H2O and NH3 in units of mol/L. Now, when we integrate, we
    #   will get the total concentration of each (in mol/L) that have been held up
    #   (or retained) within the space volume of the catalyst.
    test05.calculateRetentionIntegral('NH3 (mol/L)',False,(1/(1-0.3309)))
    test05.calculateRetentionIntegral('H2O (mol/L)',False,(1/(1-0.3309)))

    #Both of the above methods for calculating the mass retained and converting
    #the units provide very similar results.

    #plt.plot(test05.extractResultColumns('Elapsed Time (min)')['Elapsed Time (min)'],test05.extractResultColumns('NH3 (mol/L)')['NH3 (mol/L)'],'-',test05.extractResultColumns('Elapsed Time (min)')['Elapsed Time (min)'],test05.extractResultColumns('NH3 (300,3000)')['NH3 (300,3000)'],'--')
    #plt.show()

    #Works well for 1-2 plots, can do more, but gets impossible to read
    #fig, ax1 = plt.subplots()
    #ax1.plot(test05.extractResultColumns('Elapsed Time (min)')['Elapsed Time (min)'],test05.extractResultColumns('NH3 (mol/L)-Retained')['NH3 (mol/L)-Retained'],'r-',test05.extractResultColumns('Elapsed Time (min)')['Elapsed Time (min)'],test05.extractResultColumns('TC bot sample in (K)')['TC bot sample in (K)'],'g.')
    #ax1.plot(test05.extractResultColumns('Elapsed Time (min)')['Elapsed Time (min)'],test05.extractResultColumns('NH3 (mol/L)-Retained')['NH3 (mol/L)-Retained'],'r-')
    #ax2 = ax1.twinx()
    #ax2.plot(test05.extractResultColumns('Elapsed Time (min)')['Elapsed Time (min)'],test05.extractResultColumns('NH3 (300,3000)')['NH3 (300,3000)'],'b--')
    #plt.show()

    #Works well for N plots
    #plt.figure()
    #plt.subplot(221)
    #plt.plot(test05.extractResultColumns('Elapsed Time (min)')['Elapsed Time (min)'],test05.extractResultColumns('NH3 (mol/L)-Retained')['NH3 (mol/L)-Retained'],'r-')
    #plt.subplot(222)
    #plt.plot(test05.extractResultColumns('Elapsed Time (min)')['Elapsed Time (min)'],test05.extractResultColumns('NH3 (300,3000)')['NH3 (300,3000)'],'b--')
    #plt.subplot(223)
    #plt.plot(test05.extractResultColumns('Elapsed Time (min)')['Elapsed Time (min)'],test05.extractResultColumns('TC bot sample in (K)')['TC bot sample in (K)'],'g.')
    #plt.show()

    #Combination: Multiplots on one sub-plot + other sub-plots...Works well for N plots
    #   Plan:   - User will request plots for all data (or a sub-set of data)
    #           - Iterate through data and find the range for each set
    #           - Use ranges to determine whether or not that data can be
    #                   plotted on the same y-axis
    #           - If they can, then plot them on that axis
    #           - If they can't, then use figure() and subplot() --OR-- use multiple single plots
    #           - Function to get range of values, average value, maximum value, minimum value, etc in TransientData
    #           - Plot all matching run data and bypass data on the same plot

    #       PROBLEM: this method does not allow for easy labeling of the y-axis...
    #
    #       linestyle or ls 	  [ '-' | '--' | '-.' | ':' | 'steps' | ...]
    #       color options one of {'b', 'g', 'r', 'c', 'm', 'y', 'k', 'w'}
    #           Format: third option in plot, pass string as color+line
    #                       e.g.,   'b-'  = blue solid line

    #                               'r--' = red dashed line
    #Call figure() each time you are plotting a new figure
    #       figure(num, size, dpi)
    #           num = figure number
    #           size = (w,h)  where w = width (inches) and h = hieght (inches)
    #           dpi = resolution (default = 100)
    #   Should choose (w,h) based on number of subplots requested
    #       dpi should stay 100
    #       max size ==> (8.5, 11)
    #       min size ==> (7,5)
    #
    #   NOTE: Instead of using plt.show(), store figures into temporary variables,
    #           then show() for those variables to get all images to display
    #           at the same time. Requires use of input() command at the last
    #           show() command to pause execution of script for viewing images.
    #a = plt.figure(0)
    #plt.subplot(211)
    #tuple = (test05.extractResultColumns('Elapsed Time (min)')['Elapsed Time (min)'],test05.extractResultColumns('NH3 (300,3000)')['NH3 (300,3000)'],'b--',test05.extractResultColumns('Elapsed Time (min)')['Elapsed Time (min)'],test05.extractResultColumns('TC bot sample in (K)')['TC bot sample in (K)'],'g-.')
    #Use tuples to iterate through and plot sets of info to same figure
    #plt.plot(tuple[0],tuple[1],tuple[2])
    #plt.plot(tuple[3],tuple[4],tuple[5])
    #plt.legend(['NH3 (300,3000)','TC bot sample in (K)'])  #Items must be in same order plotted
    #plt.ylabel('NH3 (300,3000)\nTC bot sample in (K)')
    #xmin = test05.getMinimum('Elapsed Time (min)')
    #xmax = test05.getMaximum('Elapsed Time (min)')
    #NOTE: May want the plotter to automatically pick ymin and ymax
    #      or use the largest range to determine which set to base y-axis from
    #ymin = test05.getMinimum('NH3 (300,3000)') - test05.getDataRange('NH3 (300,3000)')/10
    #ymax = test05.getMaximum('NH3 (300,3000)') + test05.getDataRange('NH3 (300,3000)')/10
    #plt.axis([xmin, xmax, ymin, ymax])
    #plt.subplot(212)
    #plt.plot(test05.extractResultColumns('Elapsed Time (min)')['Elapsed Time (min)'],test05.extractResultColumns('NH3 (mol/L)-Retained')['NH3 (mol/L)-Retained'],'r-')
    #plt.legend(['NH3 (mol/L)-Retained'])
    #plt.ylabel('NH3 (mol/L)-Retained')
    #plt.xlabel('Elapsed Time (min)')
    #xmin = test05.getMinimum('Elapsed Time (min)')
    #xmax = test05.getMaximum('Elapsed Time (min)')
    #ymin = test05.getMinimum('NH3 (mol/L)-Retained') - test05.getDataRange('NH3 (mol/L)-Retained')/10
    #ymax = test05.getMaximum('NH3 (mol/L)-Retained') + test05.getDataRange('NH3 (mol/L)-Retained')/10
    #plt.axis([xmin, xmax, ymin, ymax])
    #Use the 'tight' options to automatically size the figure
    #plt.tight_layout()
    #plt.savefig('test.png', bbox_inches = "tight") #MUST Always call this before show()

    #Plot a section of data
    #b = plt.figure(1)
    #plt.subplot(111)
    #xvals = test05.extractResultRows(250,300)['Elapsed Time (min)']
    #yvals = test05.extractResultRows(250,300)['NH3 (300,3000)']
    #plt.plot(xvals,yvals,'o')

    #test05.createPlot(['NH3 (300,3000)','NH3 (300,3000)[bypass]'])
    #test05.createPlot(['NH3 (mol/L)-Retained'])
    #test05.createPlot('TC bot sample in (K)')

    #test05.savePlots()
    #test05.savePlots((250,300))
    #test05.saveTimeFramePlots()

    #NOTE: the subplot args represent row,cols,plot_num
    #       plot_num cannot be larger than the product of row*cols
    #       row = number of rows of plots to show
    #       cols = number of columns of plots to show

    #NTOE: Only call the compressRows(n) function when you are ready to print information to
    #       a file for visualization or further analysis purposes. The data set will lose
    #       some accuracy when the rows are compressed.
    if h2o_comp == True:
        test05.compressRows(2)
    else:
        test05.compressRows(10)

    test05.printAlltoFile()

    ## ----- End Testing -----

## Testing for SCR NOx rate data
def testing02():
    #test = PairedTransientData("20160202-CLRK-BASFCuSSZ13-700C4h-NO+NO2SCR-60k-a1_0-bp.dat","20160202-CLRK-BASFCuSSZ13-700C4h-NO+NO2SCR-60k-a1_0-250+150C.dat")
    test = TransientData("20160308-CLRK-BASFCuSSZ13-800C16-NO2SCR-60k-a1_0-550-225C.dat")
    test.compressColumns()
    #test.displayColumnNames()
    test.retainOnlyColumns(['Elapsed Time (min)','NH3 (300,3000)', 'H2O% (20)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150,2000)', 'TC bot sample out 2 (C)', 'P bottom in (bar)', 'P bottom out (bar)'])
    #test.alignData(True, False) #Only for paired data
    test.deleteColumns(['P bottom in (bar)[bypass]','P bottom out (bar)[bypass]', 'TC bot sample out 2 (C)[bypass]'])
    #print(test.getTimeFrames())  #9 time frames for unpaired  (13 for NO2SCR)

    # NOTE: Need to add O2 concentration by frame windows back into data to keep track of oxidation.
    # Specific to the NH3Inv data, time frame index 1 has O2 concentration of 0.2 %. All other time frames
    # have O2 concentration of 10%.
    #test.appendColumnByFrame('O2%', [10,0.2,10,10,10,10,10,10,10,10])
    #test.appendColumnByFrame('O2%', [10,10,10,10,10,10,10,10,10])
    #test.appendColumnByFrame('NH3 (300,3000)[bypass]', [300,300,300,300,300,300,300,300,300])
    #test.appendColumnByFrame('N2O (100,200,300)[bypass]', [0,0,0,0,0,0,0,0,0])
    #test.appendColumnByFrame('NO (350,3000)[bypass]', [300,300,300,300,300,300,300,300,300])
    #test.appendColumnByFrame('NO2 (150,2000)[bypass]', [0,0,0,0,0,0,0,0,0])
    test.appendColumnByFrame('O2%', [10,10,10,10,10,10,10,10,10,10,10,10,10])
    test.appendColumnByFrame('NH3 (300,3000)[bypass]', [300,300,300,300,300,300,300,300,300,300,300,300,300])
    test.appendColumnByFrame('N2O (100,200,300)[bypass]', [0,0,0,0,0,0,0,0,0,0,0,0,0])
    test.appendColumnByFrame('NO (350,3000)[bypass]', [0,0,0,0,0,0,0,0,0,0,0,0,0])
    test.appendColumnByFrame('NO2 (150,2000)[bypass]', [200,200,200,200,200,200,200,200,200,200,200,200,200])

    # NOTE: You should call the 'printEquilibriaTimeFrames' before comression
    test.printEquilibriaTimeFrames()
    test.compressRows(10)
    test.printAlltoFile()
    ## ----- End Testing02 ----

## Testing for Co-Optima Data
def testing03():
    # Co-Optima data cannot be automatically paired with by-pass data since the data frames for
    # the runs and by-pass do not match exactly. Instead, we will read in each seperately and
    # combine manually.

    # Read in the bypass and run files separately

    # 1-hexene (3)
    #bypass_name = "20170807-CPTMA-MalibuTWC-SGDI-30k-1Hexene-5Cramp-lambda0_999-bp-1"
    #run_name =    "20170807-CPTMA-MalibuTWC-SGDI-30k-1Hexene-5Cramp-lambda0_999-1"

    # 1-octene (4)
    #bypass_name = "20170522-CPTMA-MalibuTWC-SGDI-30k-1-Octene-5Cramp-lambda0_999-bp-1"
    #run_name =    "20170522-CPTMA-MalibuTWC-SGDI-30k-1-Octene-5Cramp-lambda0_999-1"

    # 1-propanol (3)
    #bypass_name = "20170706-CPTMA-MalibuTWC-SGDI-30k-1Propanol-5Cramp-REPEAT-lambda0_999-bp-1"
    #run_name =    "20170706-CPTMA-MalibuTWC-SGDI-30k-1Propanol-5Cramp-REPEAT-lambda0_999-1"

    # 2-Butanone (3)
    #bypass_name = "20170411-CPTMA-MalibuTWC-SGDI-30k-2Butanone-5Cramp-lambda0_999-bp-1"
    #run_name =    "20170411-CPTMA-MalibuTWC-SGDI-30k-2Butanone-5Cramp-lambda0_999-1"

    # 2-methylpentane (3)
    #bypass_name = "20170810-CPTMA-MalibuTWC-SGDI-30k-2MethylPentane-5Cramp-lambda0_999-bp-3"
    #run_name =    "20170810-CPTMA-MalibuTWC-SGDI-30k-2MethylPentane-5Cramp-lambda0_999-3"

    # 2-pentanone (3)
    #bypass_name = "20170804-CPTMA-MalibuTWC-SGDI-30k-2Pentaone-5Cramp-lambda0_999-bp-3"
    #run_name =    "20170804-CPTMA-MalibuTWC-SGDI-30k-2Pentaone-5Cramp-lambda0_999-3"

    # 2-propanol (3)
    #bypass_name = "20170614-CPTMA-MalibuTWC-SGDI-30k-2Propanol-5Cramp-REPEAT-lambda0_999-bp-3"
    #run_name =    "20170614-CPTMA-MalibuTWC-SGDI-30k-2Propanol-5Cramp-REPEAT-lambda0_999-3"

    # anisole (3)
    #bypass_name = "20170629-CPTMA-MalibuTWC-SGDI-30k-Anisole-5Cramp-lambda0_999-bp-3"
    #run_name =    "20170629-CPTMA-MalibuTWC-SGDI-30k-Anisole-5Cramp-lambda0_999-3"

    # butylacetate (3)
    #bypass_name = "20170815-CPTMA-MalibuTWC-SGDI-30k-ButylAcetate-5Cramp-lambda0_999-bp-3"
    #run_name =    "20170815-CPTMA-MalibuTWC-SGDI-30k-ButylAcetate-5Cramp-lambda0_999-3"

    # cyclopentanone (3)
    #bypass_name = "20170511-CPTMA-MalibuTWC-SGDI-30k-Cyclopentanone-5Cramp-lambda0_999-bp-3"
    #run_name =    "20170511-CPTMA-MalibuTWC-SGDI-30k-Cyclopentanone-5Cramp-lambda0_999-3"

    # diisobutylene (3)
    #bypass_name = "20170427-CPTMA-MalibuTWC-SGDI-30k-Diisobutylene-5Cramp-lambda0_999-bp-3"
    #run_name =    "20170427-CPTMA-MalibuTWC-SGDI-30k-Diisobutylene-5Cramp-lambda0_999-3"

    # E10 (3)
    #bypass_name = "20170421-CPTMA-MalibuTWC-SGDI-30k-CH3CH2OH+iC8H18+C6H5CH3-5Cramp-lambda0_999-bp-3"
    #run_name =    "20170421-CPTMA-MalibuTWC-SGDI-30k-CH3CH2OH+iC8H18+C6H5CH3-5Cramp-lambda0_999-3"

    # ethanol (3)
    #bypass_name = "20170424-CPTMA-MalibuTWC-SGDI-30k-CH3CH2OH-5Cramp-lambda0_999-bp-3"
    #run_name =    "20170424-CPTMA-MalibuTWC-SGDI-30k-CH3CH2OH-5Cramp-lambda0_999-3"

    # ethene (3)
    #bypass_name = "20170718-CPTMA-MalibuTWC-SGDI-30k-C2H4ONLY-5Cramp-lambda0_999-bp-3"
    #run_name =    "20170718-CPTMA-MalibuTWC-SGDI-30k-C2H4ONLY-5Cramp-lambda0_999-3"

    # ethyl acetate (3)
    #bypass_name = "20170510-CPTMA-MalibuTWC-SGDI-30k-EthylAcetate-5Cramp-lambda0_999-bp-3"
    #run_name =    "20170510-CPTMA-MalibuTWC-SGDI-30k-EthylAcetate-5Cramp-lambda0_999-3"

    # furan mix (4)
    #bypass_name = "20170425-CPTMA-MalibuTWC-SGDI-30k-FuranMix-5Cramp-lambda0_999-bp-4"
    #run_name =    "20170425-CPTMA-MalibuTWC-SGDI-30k-FuranMix-5Cramp-lambda0_999-4"

    # iso-butanol (3)
    #bypass_name = "20170412-CPTMA-MalibuTWC-SGDI-30k-iBuOH-5Cramp-lambda0_999-bp-3"
    #run_name =    "20170412-CPTMA-MalibuTWC-SGDI-30k-iBuOH-5Cramp-lambda0_999-3"

    # iso-butylacetate (3)
    #bypass_name = "20170823-CPTMA-MalibuTWC-SGDI-30k-isoButylAcetate-5Cramp-lambda0_999-bp-3"
    #run_name =    "20170823-CPTMA-MalibuTWC-SGDI-30k-isoButylAcetate-5Cramp-lambda0_999-3"

    # iso-butylacetate REPEAT (same folder) (3)
    #bypass_name = "20170824-CPTMA-MalibuTWC-SGDI-30k-isoButylAcetate-REPEAT-5Cramp-lambda0_999-bp-3"
    #run_name =    "20170824-CPTMA-MalibuTWC-SGDI-30k-isoButylAcetate-REPEAT-5Cramp-lambda0_999-3"

    # iso-octane (3)
    #bypass_name = "20170518-CPTMA-MalibuTWC-SGDI-30k-isooctane-5Cramp-lambda0_999-bp-3"
    #run_name =    "20170518-CPTMA-MalibuTWC-SGDI-30k-isooctane-5Cramp-lambda0_999-3"

    # m-xylene (3)
    #bypass_name = "20170524-CPTMA-MalibuTWC-SGDI-30k-mXylene-5Cramp-lambda0_999-bp-3"
    #run_name =    "20170524-CPTMA-MalibuTWC-SGDI-30k-mXylene-5Cramp-lambda0_999-3"

    # mesitylene (3)
    #bypass_name = "20170428-CPTMA-MalibuTWC-SGDI-30k-mesitylene-5Cramp-lambda0_999-bp-3"
    #run_name =    "20170428-CPTMA-MalibuTWC-SGDI-30k-mesitylene-5Cramp-lambda0_999-3"

    # methane (3)
    #bypass_name = "20170720-CPTMA-MalibuTWC-SGDI-30k-CH4ONLY-5Cramp-lambda0_999-bp-3"
    #run_name =    "20170720-CPTMA-MalibuTWC-SGDI-30k-CH4ONLY-5Cramp-lambda0_999-3"

    # methylcyclohexane (622 same folder) (2)
    #bypass_name = "20170622-CPTMA-MalibuTWC-SGDI-30k-MCH-5Cramp-REPEAT-lambda0_999-bp-2"
    #run_name =    "20170622-CPTMA-MalibuTWC-SGDI-30k-MCH-5Cramp-REPEAT-lambda0_999-2"

    # methylcyclohexane (627 same folder) (2)
    #bypass_name = "20170627-CPTMA-MalibuTWC-SGDI-30k-MCH-5Cramp-REPEAT-lambda0_999-bp-2"
    #run_name =    "20170627-CPTMA-MalibuTWC-SGDI-30k-MCH-5Cramp-REPEAT-lambda0_999-2"

    # methylcyclopentane (3)
    #bypass_name = "20170621-CPTMA-MalibuTWC-SGDI-30k-MCP-5Cramp-REPEAT-lambda0_999-bp-3"
    #run_name =    "20170621-CPTMA-MalibuTWC-SGDI-30k-MCP-5Cramp-REPEAT-lambda0_999-3"

    # methylisobutylketone (3)
    #bypass_name = "20170822-CPTMA-MalibuTWC-SGDI-30k-MIBK-5Cramp-lambda0_999-bp-3"
    #run_name =    "20170822-CPTMA-MalibuTWC-SGDI-30k-MIBK-5Cramp-lambda0_999-3"

    # n-butanol (728 same folder) (3)
    #bypass_name = "20170728-CPTMA-MalibuTWC-SGDI-30k-nButanol-5Cramp-REPEAT-lambda0_999-bp-3"
    #run_name =    "20170728-CPTMA-MalibuTWC-SGDI-30k-nButanol-5Cramp-REPEAT-lambda0_999-3"

    # n-butanol (731 same folder) (2)
    #bypass_name = "20170731-CPTMA-MalibuTWC-SGDI-30k-nButanol-5Cramp-REPEAT-lambda0_999-bp-2"
    #run_name =    "20170731-CPTMA-MalibuTWC-SGDI-30k-nButanol-5Cramp-REPEAT-lambda0_999-2"

    # n-butanol (801 same folder) (2)
    #bypass_name = "20170801-CPTMA-MalibuTWC-SGDI-30k-nButanol-5Cramp-REPEAT-lambda0_999-bp-3"
    #run_name =    "20170801-CPTMA-MalibuTWC-SGDI-30k-nButanol-5Cramp-REPEAT-lambda0_999-3"

    # n-heptane (3)
    #bypass_name = "20170808-CPTMA-MalibuTWC-SGDI-30k-nHeptane-5Cramp-lambda0_999-bp-3"
    #run_name =    "20170808-CPTMA-MalibuTWC-SGDI-30k-nHeptane-5Cramp-lambda0_999-3"

    # n-octane (3)
    #bypass_name = "20170519-CPTMA-MalibuTWC-SGDI-30k-nOctane-5Cramp-lambda0_999-bp-3"
    #run_name =    "20170519-CPTMA-MalibuTWC-SGDI-30k-nOctane-5Cramp-lambda0_999-3"

    # propane (3)
    #bypass_name = "20170717-CPTMA-MalibuTWC-SGDI-30k-C3H8ONLY-5Cramp-lambda0_999-bp-3"
    #run_name =    "20170717-CPTMA-MalibuTWC-SGDI-30k-C3H8ONLY-5Cramp-lambda0_999-3"

    # propane (724 same folder) (3)
    #bypass_name = "20170724-CPTMA-MalibuTWC-SGDI-30k-C3H6ONLY-5Cramp-lambda0_999-bp-3"
    #run_name =    "20170724-CPTMA-MalibuTWC-SGDI-30k-C3H6ONLY-5Cramp-lambda0_999-3"

    # propane (725 same folder) (3)
    #bypass_name = "20170725-CPTMA-MalibuTWC-SGDI-30k-C3H6ONLY-5Cramp-lambda0_999-bp-3"
    #run_name =    "20170725-CPTMA-MalibuTWC-SGDI-30k-C3H6ONLY-5Cramp-lambda0_999-3"

    # toluene (3)
    bypass_name = "20170505-CPTMA-MalibuTWC-SGDI-30k-Toluene-5Cramp-lambda0_999-bp-3"
    run_name =    "20170505-CPTMA-MalibuTWC-SGDI-30k-Toluene-5Cramp-lambda0_999-3"

    bypass = TransientData(bypass_name)
    run = TransientData(run_name)

    # Next, the Ethanol and CO columns have different tolerances, but with different units.
    # To deal with this, we need to first do some column manipulation to create new columns based
    # on the ppm tolerances.
    bypass.mathOperation('CO% (1)',"*",10000,True,'CO (10000)')
    run.mathOperation('CO% (1)',"*",10000,True,'CO (10000)')

    bypass.mathOperation('Ethanol% (1)',"*",10000,True,'Ethanol (10000)')
    run.mathOperation('Ethanol% (1)',"*",10000,True,'Ethanol (10000)')

    bypass.deleteColumns(['CO% (1)','Ethanol% (1)'])
    run.deleteColumns(['CO% (1)','Ethanol% (1)'])

    # Now we can compress the columns for each AND delete columns we don't need...
    bypass.compressColumns()
    run.compressColumns()

    #bypass.displayColumnNames()
    run.displayColumnNames()

    #       NOTE: Many of these items to retain may change from file to file, which is why we have to do this manually

    # 1-hexene
    #list = ['Elapsed Time (min)','NH3 (300,3000)', 'H2O% (20)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150)', 'CO2% (20)', 'CO (500,10000)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', 'Ethanol (1000,10000)', 'Toluene (1000)', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Propylene (200,1000)', 'Ethane (1000)', '2-Pentanone 150c', 'Isobutylene (500)', 'Isopentane (500)', 'CH4 (250,3000)', 'AI 2', 'AI 41', 'AI 43', 'AI 91', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)']

    # 1-octene
    #list = ['Elapsed Time (min)','NH3 (3000,300)', 'H2O% (20)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150)', 'CO2% (20)', 'CO (500,10000)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', 'Ethanol (1000,10000)', 'Toluene (1000)', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Propylene (200,1000)', 'Ethane (1000)', '2-Pentanone 150c', 'Isobutylene (500)', 'Isopentane (500)', 'CH4 (250,3000)', 'AI 2', 'AI 55', 'AI 56', 'AI 91', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)']

    # 1-propanol
    #list = ['Elapsed Time (min)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150)', 'H2O% (20)', 'CO2% (20)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', 'Anisole 150c', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Toluene (1000)', 'Isobutylene (500)', 'Ethane (1000)', 'Propylene (200,1000)', 'Isopentane (500)', 'AI 2', 'AI 31', 'AI 57', 'AI 91', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)', 'NH3 (3000,300)', 'CO (500,10000)', 'Ethanol (1000,10000)', 'CH4 (250,3000)']

    # 2-Butanone
    #list = ['Elapsed Time (min)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150)', 'H2O% (20)', 'CO2% (20)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', 'Toluene (1000)', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Propylene (200,1000)', 'Isobutylene (500)', 'Ethane (1000)', 'Isopentane (500)', 'Methyl ethyl ketone', 'AI 2', 'AI 43', 'AI 96', 'AI 91', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)', 'NH3 (3000,300)', 'CO (500,10000)', 'Ethanol (1000,10000)', 'CH4 (250,3000)']

    # 2-methylpentane
    #list = ['Elapsed Time (min)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150)', 'H2O% (20)', 'CO2% (20)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', 'Toluene (1000)', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Propylene (200,1000)', 'Ethane (1000)', '2-Pentanone 150c', 'Isobutylene (500)', 'Isopentane (500)', 'AI 2', 'AI 41', 'AI 42', 'AI 43', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)', 'NH3 (300,3000)', 'CO (500,10000)', 'Ethanol (1000,10000)', 'CH4 (250,3000)']

    # 2-pentanone
    #list = ['Elapsed Time (min)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150)', 'H2O% (20)', 'CO2% (20)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', 'Toluene (1000)', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Propylene (200,1000)', 'Ethane (1000)', '2-Pentanone 150c', 'Isobutylene (500)', 'Isopentane (500)', 'AI 2', 'AI 42', 'AI 43', 'AI 91', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)', 'NH3 (300,3000)', 'CO (500,10000)', 'Ethanol (1000,10000)', 'CH4 (250,3000)']

    ## ------- WARNING -------- This data set has an incorrectly labeled MS column for AI 2 --> 'AI 2)'
    ## ------------------------------------- see below -----------------------------------------------
    # 2-propanol
    #list = ['Elapsed Time (min)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150)', 'H2O% (20)', 'CO2% (20)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', '2-Pentanone 150c', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Toluene (1000)', 'Isobutylene (500)', 'Ethane (1000)', 'Propylene (200,1000)', 'Isopentane (500)', 'AI 2)', 'AI 45', 'AI 57', 'AI 91', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)', 'NH3 (3000,300)', 'CO (500,10000)', 'Ethanol (1000,10000)', 'CH4 (250,3000)']

    # anisole
    #list = ['Elapsed Time (min)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150)', 'H2O% (20)', 'CO2% (20)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', 'Anisole 150c', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Toluene (1000)', 'Isobutylene (500)', 'Ethane (1000)', 'Propylene (200,1000)', 'Isopentane (500)', 'AI 2', 'AI 65', 'AI 78', 'AI 93', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)', 'NH3 (3000,300)', 'CO (500,10000)', 'Ethanol (1000,10000)', 'CH4 (250,3000)']

    # butylacetate
    #list = ['Elapsed Time (min)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150)', 'H2O% (20)', 'CO2% (20)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', 'Ethyl Acetate 150c', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Propylene (200,1000)', 'Isobutylene (500)', 'Ethane (1000)', 'Toluene (1000)', 'Isopentane (500)', 'AI 2', 'AI 43', 'AI 57', 'AI 91', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)', 'NH3 (300,3000)', 'CO (500,10000)', 'Ethanol (1000,10000)', 'CH4 (250,3000)']

    # cyclopentanone
    #list = ['Elapsed Time (min)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150)', 'H2O% (20)', 'CO2% (20)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', '2-Pentanone 150c', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Toluene (1000)', 'Isobutylene (500)', 'Ethane (1000)', 'Propylene (200,1000)', 'Isopentane (500)', 'AI 2', 'AI 43', 'AI 57', 'AI 91', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)', 'NH3 (3000,300)', 'CO (500,10000)', 'Ethanol (1000,10000)', 'CH4 (250,3000)']

    # diisobutylene
    #list = ['Elapsed Time (min)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150)', 'H2O% (20)', 'CO2% (20)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', 'Methyl ethyl ketone', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Toluene (1000)', 'Isobutylene (500)', 'Ethane (1000)', 'Propylene (200,1000)', 'Isopentane (500)', 'AI 2', 'AI 97', 'AI 57', 'AI 91', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)', 'NH3 (3000,300)', 'CO (500,10000)', 'Ethanol (1000,10000)', 'CH4 (250,3000)']

    # E10
    #list = ['Elapsed Time (min)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150)', 'H2O% (20)', 'CO2% (20)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', 'Methyl ethyl ketone', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Toluene (1000)', 'Isobutylene (500)', 'Ethane (1000)', 'Propylene (200,1000)', 'Isopentane (500)', 'AI 2', 'AI 31', 'AI 57', 'AI 91', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)', 'NH3 (3000,300)', 'CO (500,10000)', 'Ethanol (1000,10000)', 'CH4 (250,3000)']

    # ethanol
    #list = ['Elapsed Time (min)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150)', 'H2O% (20)', 'CO2% (20)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', 'Methyl ethyl ketone', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Toluene (1000)', 'Isobutylene (500)', 'Ethane (1000)', 'Propylene (200,1000)', 'Isopentane (500)', 'AI 2', 'AI 31', 'AI 57', 'AI 91', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)', 'NH3 (3000,300)', 'CO (500,10000)', 'Ethanol (1000,10000)', 'CH4 (250,3000)']

    # ethene
    #list = ['Elapsed Time (min)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150)', 'H2O% (20)', 'CO2% (20)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', 'Toluene (1000)', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Propylene (200,1000)', 'Isobutylene (500)', 'Ethane (1000)', 'Propane', 'Anisole 150c', 'AI 2', 'AI 26', 'AI 57', 'AI 91', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)', 'NH3 (300,3000)', 'CO (500,10000)', 'Ethanol (1000,10000)', 'CH4 (250,3000)']

    # ethyl acetate
    #list = ['Elapsed Time (min)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150)', 'H2O% (20)', 'CO2% (20)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', 'Ethyl Acetate 150c', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Toluene (1000)', 'Isobutylene (500)', 'Ethane (1000)', 'Propylene (200,1000)', 'Isopentane (500)', 'AI 2', 'AI 43', 'AI 57', 'AI 91', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)', 'NH3 (3000,300)', 'CO (500,10000)', 'Ethanol (1000,10000)', 'CH4 (250,3000)']

    # furan mix
    #list = ['Elapsed Time (min)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150)', 'H2O% (20)', 'CO2% (20)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', 'Methyl ethyl ketone', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Toluene (1000)', 'Isobutylene (500)', 'Ethane (1000)', 'Propylene (200,1000)', 'Isopentane (500)', 'AI 2', 'AI 82', 'AI 96', 'AI 91', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)', 'NH3 (3000,300)', 'CO (500,10000)', 'Ethanol (1000,10000)', 'CH4 (250,3000)']

    # iso-butanol
    #list = ['Elapsed Time (min)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150)', 'H2O% (20)', 'CO2% (20)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', 'Toluene (1000)', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Propylene (200,1000)', 'Isobutylene (500)', 'Ethane (1000)', 'Isopentane (500)', 'Methyl ethyl ketone', 'AI 2', 'AI 43', 'AI 96', 'AI 91', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)', 'NH3 (3000,300)', 'CO (500,10000)', 'Ethanol (1000,10000)', 'CH4 (250,3000)']

    # iso-butylacetate (and repeat)
    #list = ['Elapsed Time (min)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150)', 'H2O% (20)', 'CO2% (20)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', 'Ethyl Acetate 150c', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Propylene (200,1000)', 'Isobutylene (500)', 'Ethane (1000)', 'Toluene (1000)', 'Isopentane (500)', 'AI 2', 'AI 43', 'AI 56', 'AI 91', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)', 'NH3 (300,3000)', 'CO (500,10000)', 'Ethanol (1000,10000)', 'CH4 (250,3000)']

    # iso-octane
    #list = ['Elapsed Time (min)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150)', 'H2O% (20)', 'CO2% (20)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', '2-Pentanone 150c', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Toluene (1000)', 'Isobutylene (500)', 'Ethane (1000)', 'Propylene (200,1000)', 'Isopentane (500)', 'AI 2', 'AI 31', 'AI 57', 'AI 91', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)', 'NH3 (3000,300)', 'CO (500,10000)', 'Ethanol (1000,10000)', 'CH4 (250,3000)']

    # m-xylene
    #list = ['Elapsed Time (min)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150)', 'H2O% (20)', 'CO2% (20)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', '2-Pentanone 150c', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Toluene (1000)', 'Isobutylene (500)', 'Ethane (1000)', 'Propylene (200,1000)', 'Isopentane (500)', 'AI 2', 'AI 31', 'AI 77', 'AI 91', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)', 'NH3 (3000,300)', 'CO (500,10000)', 'Ethanol (1000,10000)', 'CH4 (250,3000)']

    # mesitylene
    #list = ['Elapsed Time (min)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150)', 'H2O% (20)', 'CO2% (20)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', 'Methyl ethyl ketone', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Toluene (1000)', 'Isobutylene (500)', 'Ethane (1000)', 'Propylene (200,1000)', 'Isopentane (500)', 'AI 2', 'AI 77', 'AI 57', 'AI 91', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)', 'NH3 (3000,300)', 'CO (500,10000)', 'Ethanol (1000,10000)', 'CH4 (250,3000)']

    # methane
    #list = ['Elapsed Time (min)', 'NO (350,3000)', 'NO2 (150)', 'Propylene (200,1000)', 'H2O% (20)', 'CO2% (20)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', 'Propane', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Anisole 150c', 'Isobutylene (500)', 'Ethane (1000)', 'Toluene (1000)', 'N2O (100,200,300)', 'AI 2', 'AI 15', 'AI 57', 'AI 91', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)', 'NH3 (3000,300)', 'CO (500,10000)', 'Ethanol (1000,10000)', 'CH4 (250,3000)']

    # methylcyclohexane
    #list = ['Elapsed Time (min)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150)', 'H2O% (20)', 'CO2% (20)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', '2-Pentanone 150c', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Toluene (1000)', 'Isobutylene (500)', 'Ethane (1000)', 'Propylene (200,1000)', 'Isopentane (500)', 'AI 2', 'AI 41', 'AI 55', 'AI 83', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)', 'NH3 (3000,300)', 'CO (500,10000)', 'Ethanol (1000,10000)', 'CH4 (250,3000)']

    # methylcyclopentane
    #list = ['Elapsed Time (min)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150)', 'H2O% (20)', 'CO2% (20)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', '2-Pentanone 150c', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Toluene (1000)', 'Isobutylene (500)', 'Ethane (1000)', 'Propylene (200,1000)', 'Isopentane (500)', 'AI 2', 'AI 56', 'AI 57', 'AI 91', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)', 'NH3 (3000,300)', 'CO (500,10000)', 'Ethanol (1000,10000)', 'CH4 (250,3000)']

    # methylisobutylketone
    #list = ['Elapsed Time (min)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150)', 'H2O% (20)', 'CO2% (20)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', '2-Pentanone 150c', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Propylene (200,1000)', 'Isobutylene (500)', 'Ethane (1000)', 'Toluene (1000)', 'Isopentane (500)', 'AI 2', 'AI 43', 'AI 57', 'AI 91', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)', 'NH3 (300,3000)', 'CO (500,10000)', 'Ethanol (1000,10000)', 'CH4 (250,3000)']

    # n-butanol
    #list = ['Elapsed Time (min)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150)', 'H2O% (20)', 'CO2% (20)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', 'Toluene (1000)', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Propylene (200,1000)', 'Ethane (1000)', 'Isopentane (500)', 'Isobutylene (500)', 'Anisole 150c', 'AI 2', 'AI 42', 'AI 57', 'AI 91', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)', 'NH3 (300,3000)', 'CO (500,10000)', 'Ethanol (1000,10000)', 'CH4 (250,3000)']

    # n-heptane
    #list = ['Elapsed Time (min)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150)', 'H2O% (20)', 'CO2% (20)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', 'Toluene (1000)', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Propylene (200,1000)', 'Ethane (1000)', '2-Pentanone 150c', 'Isobutylene (500)', 'Isopentane (500)', 'AI 2', 'AI 43', 'AI 57', 'AI 71', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)', 'NH3 (300,3000)', 'CO (500,10000)', 'Ethanol (1000,10000)', 'CH4 (250,3000)']

    # n-octane
    #list = ['Elapsed Time (min)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150)', 'H2O% (20)', 'CO2% (20)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', '2-Pentanone 150c', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Toluene (1000)', 'Isobutylene (500)', 'Ethane (1000)', 'Propylene (200,1000)', 'Isopentane (500)', 'AI 2', 'AI 43', 'AI 57', 'AI 91', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)', 'NH3 (3000,300)', 'CO (500,10000)', 'Ethanol (1000,10000)', 'CH4 (250,3000)']

    # propane
    #list = ['Elapsed Time (min)', 'NO (350,3000)', 'NO2 (150)', 'Propylene (200,1000)', 'H2O% (20)', 'CO2% (20)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', 'Propane', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Anisole 150c', 'Isobutylene (500)', 'Ethane (1000)', 'Toluene (1000)', 'N2O (100,200,300)', 'AI 2', 'AI 15', 'AI 30', 'AI 43', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)', 'NH3 (3000,300)', 'CO (500,10000)', 'Ethanol (1000,10000)', 'CH4 (250,3000)']

    # propene
    #list = ['Elapsed Time (min)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150)', 'H2O% (20)', 'CO2% (20)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', 'Toluene (1000)', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Propylene (200,1000)', 'Ethane (1000)', 'Propane', 'Isobutylene (500)', 'Anisole 150c', 'AI 2', 'AI 39', 'AI 41', 'AI 42', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)', 'NH3 (300,3000)', 'CO (500,10000)', 'Ethanol (1000,10000)', 'CH4 (250,3000)']

    # toluene
    list = ['Elapsed Time (min)', 'N2O (100,200,300)', 'NO (350,3000)', 'NO2 (150)', 'H2O% (20)', 'CO2% (20)', 'Formaldehyde (70)', 'Acetaldehyde (1000)', 'Methyl ethyl ketone', 'Cyclohexane (100) 150C', 'Acetylene (1000)', 'Ethylene (100,3000)', 'Toluene (1000)', 'Isobutylene (500)', 'Ethane (1000)', 'Propylene (200,1000)', 'Isopentane (500)', 'AI 2', 'AI 31', 'AI 57', 'AI 91', 'FID THC (ppm C1)', 'TC top sample in (C)', 'TC top sample mid 2 (C)', 'TC top sample out (C)', 'P tup in (bar)', 'P top out (bar)', 'NH3 (3000,300)', 'CO (500,10000)', 'Ethanol (1000,10000)', 'CH4 (250,3000)']

    bypass.retainOnlyColumns(list)
    run.retainOnlyColumns(list)

    # If not errors are reported, then both the bypass and the run will have the same columns/names at this point

    #bypass.displayColumnNames()
    #run.displayColumnNames()

    # Next, remove any negative values from observations since they are not physically relavent
    bypass.removeNegatives(list)
    run.removeNegatives(list)

    # Now, we need to use the bypass data to find the average input values for each column
    avg = {}
    for item in list:
        avg[item] = bypass.getAverage(item)

    # Next, append columns to the run using the average inlet values just calculated from the bypass
    # Have to use appendColumnByFrame even though each frame will have same values
    frame_list = [0.]*len(run.getTimeFrames())
    for item in avg:
        if item != 'Elapsed Time (min)' and item != 'TC top sample in (C)' and item != 'TC top sample mid 2 (C)' and item != 'TC top sample out (C)' and item != 'P tup in (bar)' and item != 'P top out (bar)':
            i=0
            for v in frame_list:
                frame_list[i] = avg[item]
                i+=1
            run.appendColumnByFrame(item+'[bypass]', frame_list)

    #run.displayColumnNames()

    # Now, we can apply some additional pre-processing, such as normalization or scaling of data based on the bypass
    AI_list = []
    for item in list:
        if item.split()[0] == "AI":
            AI_list.append(item)
    for item in AI_list:
        # This line will automatically override the existing AI columns and normalize them to the bypass AI columns
        # Then, for specific AI columns, we can multiply through by the ppm specific value
        # NOTE: Since the AI columns change from folder to folder, we can't really automate more than AI 2 for H2
        if item == 'AI 2' or item == 'AI 2)':
            run.mathOperation(item,"/",item+'[bypass]')
            run.mathOperation(item,"*",1670, True, 'H2 (ppm)')
            frame_list = [1670]*len(run.getTimeFrames())
            run.appendColumnByFrame('H2 (ppm)[bypass]', frame_list)
        else:
            run.mathOperation(item,"/",item+'[bypass]')

    #run.displayColumnNames()

    # AI values...
    #       2 = H2
    #       57 = iso-octane
    #       31 = ethanol
    #       91 = toluene
    #       43 = n-octane

    # Next, scale the 'FID THC (ppm C1)' column by dividing by the corresponding bypass, then multiplying by 3000
    run.mathOperation('FID THC (ppm C1)',"/",'FID THC (ppm C1)[bypass]')
    run.mathOperation('FID THC (ppm C1)',"*",3000)
    run.mathOperation('FID THC (ppm C1)[bypass]',"/",'FID THC (ppm C1)[bypass]')
    run.mathOperation('FID THC (ppm C1)[bypass]',"*",3000)

    # Next, create columns for the appropriate O2 concentrations based on Sreshtha's paper (table 4)
    #       Fuel                   O2 %
    #       ---------               -----
    #       ethanol                 0.71
    #       n-propanol              0.71
    #       iso-propanol            0.71
    #       n-butanol               0.71
    #       iso-butanol             0.71
    #       2-butanone              0.68
    #       2-pentanone             0.68
    #       cyclopentanone          0.65
    #       methylisobutylketone    0.69
    #       ethyl acetate           0.64
    #       butyl acetate           0.66
    #       iso-butyl acetate       0.66
    #       anisole                 0.63
    #       2,5-dimethylfuran
    #       (+) 2-methylfuran       0.63
    #       toluene                 0.65
    #       m-xylene                0.66
    #       mesitylene              0.66
    #       ethene                  0.71
    #       propene                 0.71
    #       1-hexene                0.71
    #       1-octene                0.71
    #       diisobutylene           0.71
    #       methane                 0.86
    #       propane                 0.76
    #       n-heptane               0.74
    #       n-octane                0.73
    #       2-methylpentane         0.74
    #       iso-octane              0.73
    #       methylcyclohexane       0.71
    #       methylcyclopentane      0.71
    #       E10 (blend)
    #           65% iso-octane
    #           25% toluene
    #           10% ethanol         0.708

    frame_list = [0.65]*len(run.getTimeFrames())
    run.appendColumnByFrame('O2%', frame_list)

    # Now, create column for FID Conversion %, CO conversion %, and NOx conversion %
    run.mathOperation('FID THC (ppm C1)[bypass]',"-",'FID THC (ppm C1)', True, 'THC Conversion %')
    run.mathOperation('THC Conversion %',"/",'FID THC (ppm C1)[bypass]')
    run.mathOperation('THC Conversion %',"*",100)

    run.mathOperation('CO (500,10000)[bypass]',"-",'CO (500,10000)', True, 'CO Conversion %')
    run.mathOperation('CO Conversion %',"/",'CO (500,10000)[bypass]')
    run.mathOperation('CO Conversion %',"*",100)

    frame_list = [0.]*len(run.getTimeFrames())
    run.appendColumnByFrame('NOx (ppm)', frame_list)
    run.appendColumnByFrame('NOx (ppm)[bypass]', frame_list)
    run.mathOperation('NOx (ppm)',"+",'NO (350,3000)')
    run.mathOperation('NOx (ppm)',"+",'NO2 (150)')
    run.mathOperation('NOx (ppm)[bypass]',"+",'NO (350,3000)[bypass]')
    run.mathOperation('NOx (ppm)[bypass]',"+",'NO2 (150)[bypass]')

    run.mathOperation('NOx (ppm)[bypass]',"-",'NOx (ppm)', True, 'NOx Conversion %')
    run.mathOperation('NOx Conversion %',"/",'NOx (ppm)[bypass]')
    run.mathOperation('NOx Conversion %',"*",100)

    # Next, we will create a column for the average internal temperature. This temperature will
    # be just a simple average since each zone of the catalyst is the same length.
    run.mathOperation('TC top sample out (C)',"+",'TC top sample mid 2 (C)', True, 'Avg Internal Temp (C)')
    run.mathOperation('Avg Internal Temp (C)',"/",2)

    # At this point, we can automatically create and save some plots for visualization
    # NOTE: The time frame indexed by 1 represents the temperature ramp
    base_name = run_name.split(".")[0]

    run.createPlot('THC Conversion %', range=run.getTimeFrames()[1], display=False, save=True, file_name=base_name+"--THC_Conv",file_type=".png",subdir="",x_col='TC top sample in (C)')
    run.createPlot('CO Conversion %', range=run.getTimeFrames()[1], display=False, save=True, file_name=base_name+"--CO_Conv",file_type=".png",subdir="",x_col='TC top sample in (C)')
    run.createPlot('NOx Conversion %', range=run.getTimeFrames()[1], display=False, save=True, file_name=base_name+"--NOx_Conv",file_type=".png",subdir="",x_col='TC top sample in (C)')

    # At this point, we would attempt to calculate rate information (prior to row compression)

    # May also want to calculate different T-n values and print to another file


    # Lastly, we will compress the rows and print the data to a file
    run.compressRows(10)
    run.printAlltoFile()  #May want to change name and some data when calculting the rate information


    ## --- End Testing03 ----


##Directs python to call the testing function
if __name__ == "__main__":
   testing03()
